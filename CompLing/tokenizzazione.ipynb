{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizzazione"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diamo una dimostrazione pratica delle nozioni che abbiamo discusso a lezione sulla **tokenizzazione**. Verifichiamo come due grandi librerie per Python e una web app (UDPipe) gestiscono la tokenizzazione, utilizzando sia approcci \"rule based\" sia modelli addestrati su dati."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La libreria [Natural Language ToolKit (NLTK)](https://www.nltk.org/) è una piattaforma per il trattamento del linguaggio in Python.\n",
    "\n",
    "Se volete saperne di più, consiglio la lettura del bellissimo libro *Natural Language Processing with Python* di S. Bird et al. (disponibile gratuitamente [qui](https://www.nltk.org/book/))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/francesco.mambrini/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizzazione"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usiamo la semplice funzione `word_tokenize` con la frase di esempio usata nelle slide (leggermente modificata):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['That',\n",
       " 'U.S.A',\n",
       " 'poster-print',\n",
       " 'costs',\n",
       " '$',\n",
       " '12.40',\n",
       " '...',\n",
       " 'Do',\n",
       " \"n't\",\n",
       " 'buy',\n",
       " 'it',\n",
       " '!']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = \"That U.S.A poster-print costs $12.40... Don't buy it!\"\n",
    "nltk.word_tokenize(txt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notare che:\n",
    "1. la funzione esegue la semplice tokenizzazione; le frasi non sono separate\n",
    "2. `don't` viene tokenizzato in 2: [\"do\", \"n't\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come funziona? Ecco un frammento del codice (Python) del tokenizzatore. Suona familiare?\n",
    "\n",
    "```python\n",
    "class NLTKWordTokenizer(TokenizerI):\n",
    "    # ...\n",
    "\n",
    "    # Starting quotes.\n",
    "    STARTING_QUOTES = [\n",
    "        (re.compile(\"([«“‘„]|[`]+)\", re.U), r\" \\1 \"),\n",
    "        (re.compile(r\"^\\\"\"), r\"``\"),\n",
    "        (re.compile(r\"(``)\"), r\" \\1 \"),\n",
    "        (re.compile(r\"([ \\(\\[{<])(\\\"|\\'{2})\"), r\"\\1 `` \"),\n",
    "        (re.compile(r\"(?i)(\\')(?!re|ve|ll|m|t|s|d|n)(\\w)\\b\", re.U), r\"\\1 \\2\"),\n",
    "    ]\n",
    "\n",
    "    # Ending quotes.\n",
    "    ENDING_QUOTES = [\n",
    "        (re.compile(\"([»”’])\", re.U), r\" \\1 \"),\n",
    "        (re.compile(r\"''\"), \" '' \"),\n",
    "        (re.compile(r'\"'), \" '' \"),\n",
    "        (re.compile(r\"([^' ])('[sS]|'[mM]|'[dD]|') \"), r\"\\1 \\2 \"),\n",
    "        (re.compile(r\"([^' ])('ll|'LL|'re|'RE|'ve|'VE|n't|N'T) \"), r\"\\1 \\2 \"),\n",
    "    ]\n",
    "# ...\n",
    "    PUNCTUATION = [\n",
    "        (re.compile(r'([^\\.])(\\.)([\\]\\)}>\"\\'' \"»”’ \" r\"]*)\\s*$\", re.U), r\"\\1 \\2 \\3 \"),\n",
    "        (re.compile(r\"([:,])([^\\d])\"), r\" \\1 \\2\"),\n",
    "        (re.compile(r\"([:,])$\"), r\" \\1 \"),\n",
    "        (\n",
    "            re.compile(r\"\\.{2,}\", re.U),\n",
    "            r\" \\g<0> \",\n",
    "        ),  # See https://github.com/nltk/nltk/pull/2322\n",
    "        (re.compile(r\"[;@#$%&]\"), r\" \\g<0> \"),\n",
    "        (\n",
    "            re.compile(r'([^\\.])(\\.)([\\]\\)}>\"\\']*)\\s*$'),\n",
    "            r\"\\1 \\2\\3 \",\n",
    "        ),  # Handles the final period.\n",
    "        (re.compile(r\"[?!]\"), r\" \\g<0> \"),\n",
    "        (re.compile(r\"([^'])' \"), r\"\\1 ' \"),\n",
    "        (\n",
    "            re.compile(r\"[*]\", re.U),\n",
    "            r\" \\g<0> \",\n",
    "        ),  # See https://github.com/nltk/nltk/pull/2322\n",
    "    ]\n",
    "\n",
    "    # Pads parentheses\n",
    "    PARENS_BRACKETS = (re.compile(r\"[\\]\\[\\(\\)\\{\\}\\<\\>]\"), r\" \\g<0> \")\n",
    "\n",
    "    # Optionally: Convert parentheses, brackets and converts them to PTB symbols.\n",
    "    CONVERT_PARENTHESES = [\n",
    "        (re.compile(r\"\\(\"), \"-LRB-\"),\n",
    "        (re.compile(r\"\\)\"), \"-RRB-\"),\n",
    "        (re.compile(r\"\\[\"), \"-LSB-\"),\n",
    "        (re.compile(r\"\\]\"), \"-RSB-\"),\n",
    "        (re.compile(r\"\\{\"), \"-LCB-\"),\n",
    "        (re.compile(r\"\\}\"), \"-RCB-\"),\n",
    "    ]\n",
    "\n",
    "\n",
    "    # ...\n",
    "\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proprio così: sono una serie di `regular expression`! Trasformano, ad es., le classi di segni di interpunzione attaccati alle parole in: parola + spazio + segno di interpunzione.\n",
    "\n",
    "In questo modo, la tokenizzazione diventa una semplice separazione in spazi bianchi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence splitting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E il sentence splitting? Esistono funzioni apposite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- FRASE: Ciao, sono frase 1, ma puoi chiamarmi Sig.\n",
      "- FRASE: Frase.\n",
      "- FRASE: Tu come ti chiami?\n"
     ]
    }
   ],
   "source": [
    "testo = \"Ciao, sono frase 1, ma puoi chiamarmi Sig.ra Frase. Tu come ti chiami?\"\n",
    "for frase in nltk.sent_tokenize(testo):\n",
    "    print('- FRASE:', frase)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import SVG, display"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Spacy](https://spacy.io/) è una libreria Python molto moderna ed efficiente, ottimizzata per il lavoro di NLP su numerose lingue. Spacy sta rapidamente sostituendo NLTK, ma in realtà le due librerie fanno cose abbastanza diverse e possono tranquillamente coesistere..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy consente di eseguire \"catene\" di operazione di NLP (in un flusso di lavoro chiamato \"pipeline\" su cui torneremo). Il flusso è ben visualizzato da questa immagine sul [sito web](https://spacy.io/usage/processing-pipelines) della libreria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"1155\" height=\"221\" viewBox=\"0 0 1155 221\">\n  <defs>\n    <rect id=\"a\" width=\"735\" height=\"170\" x=\"210\" y=\"25\" rx=\"30\"/>\n    <mask id=\"b\" width=\"735\" height=\"170\" x=\"0\" y=\"0\" fill=\"#fff\" maskContentUnits=\"userSpaceOnUse\" maskUnits=\"objectBoundingBox\">\n      <use xlink:href=\"#a\"/>\n    </mask>\n  </defs>\n  <g fill=\"none\" fill-rule=\"evenodd\" transform=\"translate(0 26)\">\n    <rect width=\"145\" height=\"80\" x=\"2.5\" y=\"2.5\" fill=\"#D8D8D8\" stroke=\"#6A6A6A\" stroke-width=\"5\" rx=\"10\" transform=\"translate(0 70)\"/>\n    <path fill=\"#3D4251\" fill-rule=\"nonzero\" d=\"M55.4 99.7v3.9h-7.6V125H43v-21.4h-7.7v-3.9h20zm10.2 7c1 0 2.1.2 3 .6a6.8 6.8 0 014.1 4.1 9.6 9.6 0 01.6 4.3l-.2.5-.3.3H61.3c0 2 .6 3.3 1.4 4.1.9.9 2 1.3 3.5 1.3a6 6 0 001.8-.2l1.3-.6 1-.5.8-.3c.2 0 .3 0 .5.2l.3.2 1.3 1.6c-.5.6-1 1-1.6 1.4a9 9 0 01-3.9 1.4l-2 .2c-1.2 0-2.3-.2-3.4-.7-1-.4-2-1-2.8-1.8a8.6 8.6 0 01-1.9-3 11.6 11.6 0 010-7.6c.3-1.1.9-2 1.6-2.8a8 8 0 012.7-2 9 9 0 013.7-.6zm0 3.2a4 4 0 00-3 1c-.6.7-1 1.8-1.3 3h8.1c0-.5 0-1-.2-1.5-.1-.5-.4-1-.7-1.3-.3-.4-.7-.7-1.2-1a4 4 0 00-1.7-.2zm15.5 5.8l-5.9-8.7h4.2c.3 0 .5 0 .7.2l.4.4 3.7 6a4.9 4.9 0 01.6-1.2l3-4.7.4-.5.6-.2h4l-6 8.5L93 125h-4.2c-.3 0-.5 0-.7-.2l-.5-.6-3.8-6.3-.4 1.1-3.4 5.2-.5.5a1 1 0 01-.7.3H75l6-9.3zm20.5 9.6c-1.5 0-2.7-.5-3.5-1.3a5 5 0 01-1.3-3.7v-10H95c-.3 0-.5 0-.6-.2-.2-.2-.3-.4-.3-.7v-1.7l2.9-.5 1-5c0-.1 0-.3.2-.5l.7-.2h2.2v5.7h4.7v3h-4.7v9.8c0 .6.2 1 .4 1.3.3.3.7.5 1.2.5l.6-.1a3.7 3.7 0 00.9-.4l.3-.1.3.1.3.3 1.2 2c-.6.6-1.3 1-2.1 1.3a8 8 0 01-2.6.4z\"/>\n    <rect width=\"145\" height=\"80\" x=\"2.5\" y=\"2.5\" fill=\"#D7CCF4\" stroke=\"#8978B5\" stroke-width=\"5\" rx=\"10\" transform=\"translate(1005 70)\"/>\n    <path fill=\"#3D4251\" fill-rule=\"nonzero\" d=\"M1050.3 101.5a58.8 58.8 0 016.8-.4c2.2 0 4 .4 5.4 1 1.4.6 2.5 1.5 3.4 2.6a10 10 0 011.7 4 23.2 23.2 0 010 9.6c-.3 1.5-1 2.9-1.8 4-.8 1.3-2 2.2-3.5 3-1.5.7-3.4 1-5.8 1a37.3 37.3 0 01-5-.1l-1.2-.2v-24.5zm7 4a15.6 15.6 0 00-2.3 0V122h.5a158 158 0 001.6.1 6 6 0 003.2-.7c.8-.5 1.4-1.2 1.8-2 .4-.8.7-1.8.8-2.8a27.3 27.3 0 000-5.8 8 8 0 00-.7-2.6c-.4-.8-1-1.5-1.8-2-.7-.5-1.8-.8-3.1-.8zm13.4 11.8c0-1.5.2-2.8.7-4a8 8 0 014.8-4.7c1.1-.4 2.4-.6 3.8-.6 1.5 0 2.8.2 4 .7 1 .4 2 1 2.9 1.8.8.9 1.4 1.8 1.8 3 .4 1.1.6 2.4.6 3.7 0 1.5-.2 2.8-.7 4a8 8 0 01-4.8 4.7c-1.1.4-2.4.6-3.8.6a11 11 0 01-4-.7c-1-.4-2-1-2.9-1.8a7.9 7.9 0 01-1.8-3c-.4-1.1-.6-2.4-.6-3.8zm4.7 0c0 .7.1 1.4.3 2 .2.7.5 1.3 1 1.8a4.1 4.1 0 003.3 1.5c1.4 0 2.5-.4 3.3-1.3.9-.8 1.3-2.2 1.3-4a6 6 0 00-1.2-4c-.8-1-2-1.4-3.4-1.4-.7 0-1.3 0-1.8.3-.6.2-1 .5-1.5 1-.4.4-.7 1-1 1.6-.2.7-.3 1.5-.3 2.4zm34.2 7c-1 .7-2 1.3-3.3 1.6-1.3.4-2.7.6-4 .6-1.6 0-3-.2-4.1-.7-1.2-.4-2.2-1-3-1.8a8 8 0 01-1.8-3 10.9 10.9 0 010-7.7 8.2 8.2 0 015.2-4.7 14.3 14.3 0 017.6-.2l2.6 1v6.1h-3.8v-3.2l-2.2-.3c-.7 0-1.3.1-2 .3a4.8 4.8 0 00-2.9 2.6c-.3.7-.5 1.4-.5 2.3 0 .8.2 1.5.4 2.1a5 5 0 002.8 2.8 8.2 8.2 0 005.6-.2l1.9-1 1.5 3.4z\"/>\n    <use stroke=\"#3AC\" stroke-dasharray=\"5 10\" stroke-width=\"10\" mask=\"url(#b)\" xlink:href=\"#a\"/>\n    <g transform=\"translate(540)\">\n      <rect width=\"95\" height=\"50\" x=\"2.5\" y=\"2.5\" fill=\"#C3E7F1\" stroke=\"#3AC\" stroke-width=\"5\" rx=\"10\"/>\n      <path fill=\"#3D4251\" fill-rule=\"nonzero\" d=\"M27.8 24.5h4.4l.3 1.6h.1a5.2 5.2 0 014.2-2c.7 0 1.3.1 1.8.3.6.2 1 .4 1.4.8.4.4.7 1 1 1.6.1.6.3 1.5.3 2.4V37H38v-7.1c0-1-.2-1.8-.7-2.2-.4-.5-1-.7-1.7-.7-.6 0-1.2.2-1.7.6-.5.3-.9.8-1 1.3V37h-3.3v-9.8h-1.8v-2.7zm16.9-5H50v11.6c0 1.2.2 2.1.5 2.6s.8.8 1.5.8c.5 0 1 0 1.3-.2l1-.4 1.2 2.2a15.3 15.3 0 01-1.8 1 6.1 6.1 0 01-2.3.3c-1.5 0-2.7-.4-3.5-1.3-.8-.8-1.1-1.9-1.1-3.4V22.3h-2.1v-2.7zm12.8 5h4.3L62 26h.1c.9-1.2 2.3-1.9 4.2-1.9a6 6 0 012.1.4c.7.3 1.2.6 1.7 1.1.4.6.8 1.2 1 2 .3.8.4 1.7.4 2.8 0 1-.1 2-.4 3-.3.8-.7 1.5-1.2 2.1-.6.6-1.2 1-2 1.4-.7.3-1.6.5-2.6.5-.5 0-1 0-1.5-.2-.5 0-1-.2-1.3-.3V42h-3.2V27.2h-1.9v-2.7zm8 2.4c-.7 0-1.3.2-1.8.5s-.9.8-1 1.4V34c.2.2.5.3 1 .4l1.3.2c.4 0 .9 0 1.3-.2s.7-.4 1-.8c.3-.4.6-.8.7-1.3.2-.6.3-1.2.3-2 0-1-.3-1.9-.8-2.5-.6-.6-1.2-.9-2-.9z\"/>\n    </g>\n    <path fill=\"#3AC\" d=\"M205 112.5L180 125v-25z\"/>\n    <path stroke=\"#3AC\" stroke-linecap=\"square\" stroke-width=\"5\" d=\"M180 112.5h-23.1\"/>\n    <path fill=\"#3AC\" d=\"M1000 112.5L975 125v-25z\"/>\n    <path stroke=\"#3AC\" stroke-linecap=\"square\" stroke-width=\"5\" d=\"M975 112.5h-23.1\"/>\n    <path fill=\"#EAC1CC\" stroke=\"#F03969\" stroke-linejoin=\"round\" stroke-width=\"3.8\" d=\"M230 75h135l23.5 43.4L365 160H230l23.5-41.5z\"/>\n    <path fill=\"#F2D7B2\" stroke=\"#F0A439\" stroke-linejoin=\"round\" stroke-width=\"3.8\" d=\"M395 75h135l23.5 43.4L530 160H395l23.5-41.5z\"/>\n    <path fill=\"#F2E7A6\" stroke=\"#CDB217\" stroke-linejoin=\"round\" stroke-width=\"3.8\" d=\"M515 75h135l23.5 43.4L650 160H515l23.5-41.5z\"/>\n    <path fill=\"#D7E99A\" stroke=\"#B2D73A\" stroke-linejoin=\"round\" stroke-width=\"3.8\" d=\"M640 75h135l23.5 43.4L775 160H640l23.5-41.5z\"/>\n    <path fill=\"#B5F3D4\" stroke=\"#3AD787\" stroke-linejoin=\"round\" stroke-width=\"3.8\" d=\"M765 75h135l23.5 43.4L900 160H765l23.5-41.5z\"/>\n    <path fill=\"#3D4251\" fill-rule=\"nonzero\" d=\"M265.9 125.2c-1.1 0-2-.3-2.6-1-.6-.6-.9-1.4-.9-2.5v-7.2h-1.3c-.2 0-.3 0-.4-.2-.2 0-.2-.2-.2-.5v-1.2l2-.3.7-3.5.2-.4.5-.2h1.6v4h3.4v2.3h-3.4v7c0 .3 0 .6.3.9.2.2.5.3.8.3h.5a2.6 2.6 0 00.6-.3l.2-.1h.2l.2.3 1 1.5-1.6.8-1.8.3zm10.9-13.2c1 0 1.8.1 2.6.4a5.6 5.6 0 013.3 3.4c.3.8.4 1.8.4 2.8 0 1-.1 1.9-.4 2.7a5.5 5.5 0 01-3.3 3.4 7 7 0 01-2.6.5 7 7 0 01-2.6-.5 5.6 5.6 0 01-3.3-3.4 7.8 7.8 0 010-5.5c.3-.8.7-1.5 1.3-2 .5-.6 1.2-1 2-1.4a7 7 0 012.6-.4zm0 10.8c1 0 1.9-.3 2.4-1 .5-.8.7-1.8.7-3.2 0-1.4-.2-2.4-.7-3.2-.5-.7-1.3-1-2.4-1-1 0-1.9.3-2.4 1-.5.8-.8 1.8-.8 3.2 0 1.4.3 2.4.8 3.1.5.8 1.3 1.1 2.4 1.1zm11.9-16.4v10.7h.5l.5-.1.4-.3 3.2-4 .4-.4.7-.1h2.8l-4 4.7-.4.5-.5.4.4.4.4.6 4.3 6.2h-2.8l-.6-.1c-.2-.1-.3-.2-.4-.5l-3.3-4.8a1 1 0 00-.4-.4h-1.2v5.8h-3.1v-18.6h3zm16 5.6c.7 0 1.5.1 2.2.4a4.9 4.9 0 012.9 3 6.9 6.9 0 01.3 3v.3l-.3.2h-8.3c.1 1.4.5 2.4 1.1 3 .6.6 1.4.9 2.4.9.6 0 1 0 1.3-.2a22 22 0 001.7-.8l.6-.1h.3l.3.3.9 1c-.4.5-.8.8-1.2 1a6.4 6.4 0 01-2.7 1c-.5.2-1 .2-1.4.2-1 0-1.7-.2-2.5-.5s-1.4-.7-2-1.3c-.6-.5-1-1.3-1.4-2.1a8.3 8.3 0 010-5.5 5.7 5.7 0 013.2-3.4c.7-.3 1.6-.4 2.5-.4zm0 2.2c-1 0-1.6.2-2.1.8-.5.5-.9 1.2-1 2.1h5.8c0-.4 0-.8-.2-1.1 0-.4-.2-.7-.5-1l-.8-.6-1.2-.2zm8 10.8v-12.8h1.9c.4 0 .6.2.8.5l.2 1a7 7 0 011.7-1.2 4.6 4.6 0 012.2-.5c.7 0 1.4 0 1.9.3l1.4 1 .8 1.6c.2.6.3 1.2.3 2v8.1h-3.1v-8.2c0-.7-.2-1.4-.6-1.8-.3-.4-.9-.6-1.6-.6l-1.5.3c-.5.3-1 .6-1.3 1v9.3h-3.1zm17.5-12.8V125H327v-12.8h3zm.4-3.8l-.1.8a2 2 0 01-1 1 2 2 0 01-2.2-.4 2 2 0 01-.4-.6l-.2-.8a2 2 0 01.6-1.4 2 2 0 011.3-.5l.8.1a2 2 0 011 1l.3.8zm12.3 5v.7l-.3.5-6.2 8h6.4v2.4h-10v-1.3l.2-.5c0-.2.1-.4.3-.5l6.1-8.2h-6.2v-2.3h9.8v1.3zm7.8-1.4c.8 0 1.6.1 2.2.4a4.9 4.9 0 013 3 6.9 6.9 0 01.3 3v.3l-.3.2h-8.3c.1 1.4.5 2.4 1 3 .7.6 1.5.9 2.5.9.5 0 1 0 1.3-.2a22 22 0 001.7-.8l.6-.1h.3l.3.3.8 1c-.3.5-.7.8-1.1 1a6.4 6.4 0 01-2.7 1c-.5.2-1 .2-1.4.2-1 0-1.8-.2-2.5-.5-.8-.3-1.5-.7-2-1.3-.6-.5-1-1.3-1.4-2.1a8.3 8.3 0 010-5.5 5.7 5.7 0 013.2-3.4c.7-.3 1.6-.4 2.5-.4zm0 2.2c-.8 0-1.5.2-2 .8-.5.5-.9 1.2-1 2.1h5.8c0-.4 0-.8-.2-1.1 0-.4-.2-.7-.5-1l-.8-.6-1.2-.2zm8 10.8v-12.8h1.9l.6.1c.2.2.3.4.3.7l.2 1.5a6 6 0 011.6-1.9c.6-.4 1.3-.7 2-.7s1.2.2 1.6.5l-.3 2.3-.2.3-.3.1h-.6l-.8-.2c-.7 0-1.2.2-1.7.6a4 4 0 00-1.1 1.5v8h-3.1z\"/>\n    <path fill=\"#3D4251\" fill-rule=\"nonzero\" d=\"M440.9 125.2c-1.1 0-2-.3-2.6-1-.6-.6-.9-1.4-.9-2.5v-7.2h-1.3c-.2 0-.3 0-.4-.2-.2 0-.2-.2-.2-.5v-1.2l2-.3.7-3.5.2-.4.5-.2h1.6v4h3.4v2.3h-3.4v7c0 .3 0 .6.3.9.2.2.5.3.8.3h.5a2.6 2.6 0 00.6-.3l.2-.1h.2l.2.3 1 1.5-1.6.8-1.8.3zm15.5-.2H455l-.7-.1c-.2-.1-.3-.3-.4-.6l-.3-.9a10.6 10.6 0 01-1.9 1.3 5 5 0 01-1 .4 6.4 6.4 0 01-2.8-.1l-1.2-.7a3 3 0 01-.7-1c-.2-.5-.3-1-.3-1.6 0-.5.1-1 .4-1.4.2-.5.6-.9 1.2-1.3s1.4-.7 2.4-1c1-.2 2.2-.3 3.7-.3v-.8c0-.9-.2-1.5-.6-2-.3-.3-.9-.5-1.6-.5a3.8 3.8 0 00-2 .5l-.8.4c-.2.2-.4.2-.6.2-.3 0-.4 0-.6-.2l-.3-.3-.6-1c1.5-1.4 3.2-2 5.3-2 .8 0 1.4 0 2 .3a4.3 4.3 0 012.5 2.6c.2.6.3 1.3.3 2v8.1zm-6-2h.9a3.3 3.3 0 001.4-.7l.7-.6v-2.2c-1 0-1.7.1-2.3.3a6 6 0 00-1.5.4l-.7.6c-.2.2-.3.5-.3.8 0 .5.2.9.5 1.1.3.3.8.4 1.3.4zm13.5-11l1.5.1 1.3.5h3.7v1.2l-.1.4-.6.2-1.1.3a4 4 0 01.3 1.4 3.8 3.8 0 01-1.5 3c-.4.4-1 .7-1.6.9a6.5 6.5 0 01-3.4.1c-.4.3-.6.5-.6.8 0 .3.2.5.4.6l1 .3h1.3a27.5 27.5 0 013 .3l1.3.5 1 1c.2.3.3.8.3 1.5 0 .5-.2 1-.4 1.6-.3.5-.7 1-1.2 1.4-.6.4-1.2.8-2 1a10.1 10.1 0 01-5.2.1 6 6 0 01-1.7-.7c-.5-.3-.9-.7-1-1.1-.3-.4-.4-.8-.4-1.3 0-.6.1-1 .5-1.5.4-.4.9-.7 1.5-1-.3-.1-.5-.4-.7-.7a2 2 0 01-.3-1.1v-.6l.4-.6.5-.6.8-.5a3.7 3.7 0 01-2-3.5 3.8 3.8 0 011.3-3l1.6-.8c.6-.2 1.3-.3 2-.3zm3.3 13.6c0-.3 0-.5-.2-.6-.1-.2-.3-.3-.6-.4l-1-.2a16.7 16.7 0 00-2.2-.2H462c-.4.1-.6.4-.8.6-.3.3-.4.6-.4 1 0 .2 0 .4.2.6l.5.5 1 .3 1.4.1 1.5-.1c.4-.1.8-.2 1-.4l.7-.5.1-.7zm-3.3-7.3c.3 0 .7 0 1-.2l.7-.4.4-.7.1-.8a2 2 0 00-.5-1.5c-.4-.4-1-.6-1.8-.6-.7 0-1.3.2-1.7.6a2 2 0 00-.5 1.5l.1.8a1.8 1.8 0 001.2 1.1l1 .2zm12.9-6.3l1.5.1 1.4.5h3.7v1.2l-.2.4-.5.2-1.2.3a4 4 0 01.3 1.4 3.8 3.8 0 01-1.4 3c-.5.4-1 .7-1.6.9a6.5 6.5 0 01-3.4.1c-.4.3-.6.5-.6.8 0 .3 0 .5.3.6l1 .3h1.3a27.5 27.5 0 013 .3l1.3.5 1 1c.2.3.3.8.3 1.5 0 .5-.1 1-.4 1.6-.3.5-.7 1-1.2 1.4-.5.4-1.2.8-2 1a10.1 10.1 0 01-5.2.1 6 6 0 01-1.7-.7c-.5-.3-.8-.7-1-1.1-.3-.4-.4-.8-.4-1.3 0-.6.2-1 .5-1.5.4-.4 1-.7 1.6-1-.3-.1-.6-.4-.8-.7a2 2 0 01-.3-1.1l.1-.6.3-.6.6-.6.7-.5a3.7 3.7 0 01-2-3.5 3.8 3.8 0 011.3-3c.5-.3 1-.6 1.7-.8.6-.2 1.3-.3 2-.3zm3.4 13.6c0-.3-.1-.5-.3-.6-.1-.2-.3-.3-.6-.4l-.9-.2a16.7 16.7 0 00-2.3-.2H475l-.8.6c-.2.3-.3.6-.3 1l.1.6.6.5 1 .3 1.4.1 1.5-.1 1-.4c.3-.1.5-.3.6-.5l.2-.7zm-3.4-7.3c.4 0 .7 0 1-.2.3 0 .5-.2.7-.4l.4-.7.2-.8a2 2 0 00-.6-1.5c-.4-.4-1-.6-1.7-.6-.8 0-1.3.2-1.7.6a2 2 0 00-.6 1.5c0 .3 0 .6.2.8a1.8 1.8 0 001 1.1l1 .2zm13.8-6.3c.8 0 1.5.1 2.2.4a4.9 4.9 0 013 3 6.9 6.9 0 01.3 3l-.1.3-.2.2h-8.3c0 1.4.4 2.4 1 3 .7.6 1.5.9 2.5.9.5 0 1 0 1.3-.2a22 22 0 001.7-.8l.6-.1h.3l.2.3 1 1c-.4.5-.8.8-1.2 1a6.4 6.4 0 01-2.8 1c-.4.2-1 .2-1.4.2-.9 0-1.7-.2-2.4-.5-.8-.3-1.5-.7-2-1.3-.6-.5-1-1.3-1.4-2.1a8.3 8.3 0 010-5.5 5.7 5.7 0 013.2-3.4c.7-.3 1.5-.4 2.5-.4zm0 2.2c-.9 0-1.6.2-2 .8-.6.5-.9 1.2-1 2.1h5.8c0-.4 0-.8-.2-1.1l-.5-1-.8-.6-1.3-.2zm8 10.8v-12.8h1.9l.6.1c.2.2.2.4.3.7l.2 1.5a6 6 0 011.6-1.9c.6-.4 1.3-.7 2-.7s1.2.2 1.6.5l-.4 2.3-.1.3-.4.1h-.5l-.8-.2c-.7 0-1.2.2-1.7.6a4 4 0 00-1.2 1.5v8h-3z\"/>\n    <path fill=\"#3D4251\" fill-rule=\"nonzero\" d=\"M556.6 129.2v-17h2l.4.1c.2.1.3.2.3.4l.3 1.2c.5-.6 1-1 1.8-1.4a4.8 4.8 0 014.2-.1c.6.3 1.1.7 1.5 1.2a6 6 0 011 2 10.3 10.3 0 010 5.6c-.3.8-.7 1.5-1.1 2a5.1 5.1 0 01-6 1.7l-1.3-1v5.3h-3zm6-14.8c-.6 0-1.1.1-1.6.4-.4.3-.9.6-1.3 1.1v5.8a3 3 0 002.5 1.1c.5 0 .9 0 1.3-.2l1-.8c.2-.4.4-.8.5-1.4a8.6 8.6 0 000-3.8c0-.5-.2-1-.4-1.3a2 2 0 00-.9-.7c-.3-.2-.6-.2-1-.2zm18.2 10.6h-1.3l-.7-.1c-.2-.1-.3-.3-.4-.6l-.3-.9a10.6 10.6 0 01-2 1.3 5 5 0 01-1 .4 6.4 6.4 0 01-2.7-.1c-.5-.2-.9-.4-1.2-.7a3 3 0 01-.8-1c-.2-.5-.3-1-.3-1.6 0-.5.2-1 .4-1.4.3-.5.7-.9 1.3-1.3.6-.4 1.4-.7 2.4-1 1-.2 2.2-.3 3.6-.3v-.8c0-.9-.2-1.5-.5-2-.4-.3-1-.5-1.6-.5a3.8 3.8 0 00-2.1.5l-.7.4c-.2.2-.4.2-.7.2-.2 0-.4 0-.5-.2-.2 0-.3-.2-.4-.3l-.5-1c1.4-1.4 3.2-2 5.3-2a4.3 4.3 0 014.4 3c.2.5.3 1.2.3 1.9v8.1zm-6-2h1a3.3 3.3 0 001.4-.7l.6-.6v-2.2c-.9 0-1.6.1-2.2.3a6 6 0 00-1.5.4l-.8.6-.2.8c0 .5.2.9.5 1.1.3.3.7.4 1.2.4zm9 2v-12.8h1.9l.6.1c.2.2.3.4.3.7l.2 1.5a6 6 0 011.6-1.9c.6-.4 1.3-.7 2-.7s1.2.2 1.6.5l-.4 2.3-.1.3-.4.1h-.5l-.8-.2c-.7 0-1.2.2-1.7.6a4 4 0 00-1.1 1.5v8h-3.1zm17.9-10.3l-.3.3h-.8a32.9 32.9 0 00-1.4-.7h-1c-.6 0-1 0-1.4.3-.4.3-.5.6-.5 1 0 .3 0 .5.2.7l.7.5 1 .4a33 33 0 012.3.8c.4.2.8.4 1 .7.4.2.6.5.8 1l.2 1.2c0 .7 0 1.2-.3 1.7-.2.6-.6 1-1 1.4-.4.4-1 .7-1.6.9a7 7 0 01-3.5.2 7.6 7.6 0 01-2.3-.8l-.8-.7.7-1.1c0-.2.2-.3.3-.4h1a12 12 0 001.4.8l1.2.1h1l.6-.4c.1-.2.3-.3.3-.5l.1-.6c0-.3 0-.6-.2-.8l-.7-.5-1-.3a33.5 33.5 0 01-2.4-.9 4 4 0 01-1-.7 3 3 0 01-.7-1 3.7 3.7 0 011-4.2c.4-.3.9-.6 1.5-.8.6-.2 1.3-.3 2-.3 1 0 1.8.1 2.5.4.7.3 1.3.7 1.8 1.2l-.7 1zm8.6-2.7c.8 0 1.6.1 2.2.4a4.9 4.9 0 013 3 6.9 6.9 0 01.3 3v.3l-.3.2h-8.3c.1 1.4.5 2.4 1 3 .7.6 1.5.9 2.5.9.5 0 1 0 1.3-.2a22 22 0 001.7-.8l.6-.1h.3l.3.3.9 1c-.4.5-.8.8-1.2 1a6.4 6.4 0 01-2.7 1c-.5.2-1 .2-1.4.2-1 0-1.8-.2-2.5-.5-.8-.3-1.5-.7-2-1.3-.6-.5-1-1.3-1.4-2.1a8.3 8.3 0 010-5.5 5.7 5.7 0 013.2-3.4c.7-.3 1.6-.4 2.5-.4zm0 2.2c-.8 0-1.5.2-2 .8-.5.5-.9 1.2-1 2.1h5.8c0-.4 0-.8-.2-1.1 0-.4-.2-.7-.5-1l-.8-.6-1.2-.2zm8 10.8v-12.8h1.9l.6.1c.2.2.3.4.3.7l.2 1.5a6 6 0 011.6-1.9c.6-.4 1.3-.7 2-.7s1.2.2 1.6.5l-.4 2.3-.1.3-.4.1h-.5l-.8-.2c-.7 0-1.2.2-1.7.6a4 4 0 00-1.1 1.5v8h-3.1z\"/>\n    <path fill=\"#3D4251\" fill-rule=\"nonzero\" d=\"M701.6 125v-12.8h2c.3 0 .6.2.7.5l.2 1a7 7 0 011.8-1.2 4.6 4.6 0 012.2-.5c.7 0 1.3 0 1.9.3.5.3 1 .6 1.3 1 .4.5.7 1 .8 1.6.2.6.3 1.2.3 2v8.1h-3v-8.2c0-.7-.2-1.4-.6-1.8-.4-.4-1-.6-1.6-.6-.6 0-1 .1-1.5.3l-1.4 1v9.3h-3zm19.6-13c.8 0 1.5.1 2.2.4a4.9 4.9 0 012.9 3 6.9 6.9 0 01.4 3l-.1.3-.3.2H718c.2 1.4.5 2.4 1.1 3 .7.6 1.5.9 2.5.9.5 0 1 0 1.3-.2a22 22 0 001.7-.8l.5-.1h.4l.2.3.9 1c-.3.5-.7.8-1.1 1a6.4 6.4 0 01-2.8 1c-.5.2-1 .2-1.4.2-.9 0-1.7-.2-2.5-.5-.7-.3-1.4-.7-2-1.3-.5-.5-1-1.3-1.3-2.1a8.3 8.3 0 010-5.5 5.7 5.7 0 013.2-3.4c.6-.3 1.5-.4 2.5-.4zm0 2.2c-.9 0-1.6.2-2 .8-.6.5-1 1.2-1 2.1h5.7l-.1-1.1-.5-1-.9-.6-1.2-.2zm8 10.8v-12.8h1.8l.7.1.3.7.1 1.5a6 6 0 011.6-1.9c.7-.4 1.4-.7 2.1-.7.7 0 1.2.2 1.6.5l-.4 2.3c0 .1 0 .2-.2.3l-.3.1h-.5l-.9-.2c-.6 0-1.2.2-1.6.6a4 4 0 00-1.2 1.5v8h-3z\"/>\n    <path fill=\"#3D4251\" fill-rule=\"nonzero\" d=\"M831 123.3a2 2 0 01.5-1.3 2 2 0 011.3-.6 1.9 1.9 0 011.4.6 1.9 1.9 0 01.3 2 1.8 1.8 0 01-1 1 2 2 0 01-2-.4c-.2-.1-.3-.3-.4-.6a2 2 0 01-.2-.7zm5.5 0a2 2 0 01.6-1.3 2 2 0 011.3-.6 1.9 1.9 0 011.4.6 1.9 1.9 0 01.4 2 1.8 1.8 0 01-1 1 2 2 0 01-2-.4c-.3-.1-.4-.3-.5-.6a2 2 0 01-.2-.7zm5.7 0a2 2 0 01.5-1.3 2 2 0 011.4-.6 1.9 1.9 0 011.3.6 1.9 1.9 0 01.4 2 1.8 1.8 0 01-1 1 2 2 0 01-2-.4c-.3-.1-.4-.3-.5-.6a2 2 0 01-.1-.7z\"/>\n  </g>\n</svg>",
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(SVG(url='https://spacy.io/images/pipeline.svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/nlp/lib/python3.11/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import spacy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lavoriamo sull'italiano! Creiamo una pipeline vuota in cui specifichiamo solo il codice della lingua. Perché secondo voi dobbiamo specificarlo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank('it')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "manzoni = '''Quel ramo del lago di Como, che volge a mezzogiorno, tra due catene non interrotte di monti, tutto a seni e a golfi, a seconda dello sporgere e del rientrare di quelli, vien, quasi a un tratto, a ristringersi, e a prender corso e figura di fiume, tra un promontorio a destra, e un'ampia costiera dall'altra parte; e il ponte, che ivi congiunge le due rive, par che renda ancor più sensibile all'occhio questa trasformazione, e segni il punto in cui il lago cessa, e l'Adda rincomincia, per ripigliar poi nome di lago dove le rive, allontanandosi di nuovo, lascian l'acqua distendersi e rallentarsi in nuovi golfi e in nuovi seni. La costiera, formata dal deposito di tre grossi torrenti, scende appoggiata a due monti contigui, l'uno detto di san Martino, l'altro, con voce lombarda, il Resegone, dai molti suoi cocuzzoli in fila, che in vero lo fanno somigliare a una sega: talché non è chi, al primo vederlo, purché sia di fronte, come per esempio di su le mura di Milano che guardano a settentrione, non lo discerna tosto, a un tal contrassegno, in quella lunga e vasta giogaia, dagli altri monti di nome più oscuro e di forma più comune. Per un buon pezzo, la costa sale con un pendìo lento e continuo; poi si rompe in poggi e in valloncelli, in erte e in ispianate, secondo l'ossatura de' due monti, e il lavoro dell'acque. Il lembo estremo, tagliato dalle foci de' torrenti, è quasi tutto ghiaia e ciottoloni; il resto, campi e vigne, sparse di terre, di ville, di casali; in qualche parte boschi, che si prolungano su per la montagna. Lecco, la principale di quelle terre, e che dà nome al territorio, giace poco discosto dal ponte, alla riva del lago, anzi viene in parte a trovarsi nel lago stesso, quando questo ingrossa: un gran borgo al giorno d'oggi, e che s'incammina a diventar città. Ai tempi in cui accaddero i fatti che prendiamo a raccontare, quel borgo, già considerabile, era anche un castello, e aveva perciò l'onore d'alloggiare un comandante, e il vantaggio di possedere una stabile guarnigione di soldati spagnoli, che insegnavan la modestia alle fanciulle e alle donne del paese, accarezzavan di tempo in tempo le spalle a qualche marito, a qualche padre; e, sul finir dell'estate, non mancavan mai di spandersi nelle vigne, per diradar l'uve, e alleggerire a' contadini le fatiche della vendemmia. Dall'una all'altra di quelle terre, dall'alture alla riva, da un poggio all'altro, correvano, e corrono tuttavia, strade e stradette, più o men ripide, o piane; ogni tanto affondate, sepolte tra due muri, donde, alzando lo sguardo, non iscoprite che un pezzo di cielo e qualche vetta di monte; ogni tanto elevate su terrapieni aperti: e da qui la vista spazia per prospetti più o meno estesi, ma ricchi sempre e sempre qualcosa nuovi, secondo che i diversi punti piglian più o meno della vasta scena circostante, e secondo che questa o quella parte campeggia o si scorcia, spunta o sparisce a vicenda. Dove un pezzo, dove un altro, dove una lunga distesa di quel vasto e variato specchio dell'acqua; di qua lago, chiuso all'estremità o piùttosto smarrito in un gruppo, in un andirivieni di montagne, e di mano in mano più allargato tra altri monti che si spiegano, a uno a uno, allo sguardo, e che l'acqua riflette capovolti, co' paesetti posti sulle rive; di là braccio di fiume, poi lago, poi fiume ancora, che va a perdersi in lucido serpeggiamento pur tra' monti che l'accompagnano, degradando via via, e perdendosi quasi anch'essi nell'orizzonte. Il luogo stesso da dove contemplate que' vari spettacoli, vi fa spettacolo da ogni parte: il monte di cui passeggiate le falde, vi svolge, al di sopra, d'intorno, le sue cime e le balze, distinte, rilevate, mutabili quasi a ogni passo, aprendosi e contornandosi in gioghi ciò che v'era sembrato prima un sol giogo, e comparendo in vetta ciò che poco innanzi vi si rappresentava sulla costa: e l'ameno, il domestico di quelle falde tempera gradevolmente il selvaggio, e orna vie più il magnifico dell'altre vedute.'''\n",
    "tweet = '''8 MARZO | Al Senato il concerto in occasione delle celebrazioni della \"Giornata Internazionale della Donna\"\n",
    "#ANSA'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_exp = nlp.tokenizer.explain(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('TOKEN', '8'),\n",
       " ('TOKEN', 'MARZO'),\n",
       " ('TOKEN', '|'),\n",
       " ('TOKEN', 'Al'),\n",
       " ('TOKEN', 'Senato'),\n",
       " ('TOKEN', 'il'),\n",
       " ('TOKEN', 'concerto'),\n",
       " ('TOKEN', 'in'),\n",
       " ('TOKEN', 'occasione'),\n",
       " ('TOKEN', 'delle'),\n",
       " ('TOKEN', 'celebrazioni'),\n",
       " ('TOKEN', 'della'),\n",
       " ('PREFIX', '\"'),\n",
       " ('TOKEN', 'Giornata'),\n",
       " ('TOKEN', 'Internazionale'),\n",
       " ('TOKEN', 'della'),\n",
       " ('TOKEN', 'Donna'),\n",
       " ('SUFFIX', '\"'),\n",
       " ('PREFIX', '#'),\n",
       " ('TOKEN', 'ANSA')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_exp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il metodo `explain()` serve proprio a rendere esplicito il funzionamento del tokenizzatore! Il tokenizzatore di `spacy` funziona così:\n",
    "1. segmenta il testo usando gli **spazi bianchi**\n",
    "2. controlla i token ottenuti da questo primo passaggio e prova a separare dei \"prefissi\" o \"suffissi\" (e.g. virgole, segni di punteggiatura)\n",
    "3. continua con 2 finché nessun token può essere ulteriormente scomposto"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vediamo un esempio francese:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('TOKEN', 'Je'),\n",
       " ('PREFIX', \"l'\"),\n",
       " ('TOKEN', 'ai'),\n",
       " ('TOKEN', 'vu'),\n",
       " ('TOKEN', \"aujud'\"),\n",
       " ('TOKEN', 'hui'),\n",
       " ('SUFFIX', ';'),\n",
       " ('TOKEN', 'je'),\n",
       " ('PREFIX', \"n'\"),\n",
       " ('TOKEN', 'ai'),\n",
       " ('TOKEN', 'pas'),\n",
       " ('TOKEN', 'compris'),\n",
       " ('PREFIX', \"d'\"),\n",
       " ('TOKEN', 'où'),\n",
       " ('TOKEN', 'il'),\n",
       " ('TOKEN', 'venait')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fr = spacy.blank('fr')\n",
    "fr.tokenizer.explain(\"Je l'ai vu aujud'hui; je n'ai pas compris d'où il venait\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E il cinese? Servirà, ovviamente, una lista di regole/parole speciale!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "zh_nlp = spacy.blank('zh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "姚\n",
      "明\n",
      "进\n",
      "入\n",
      "总\n",
      "决\n",
      "赛\n"
     ]
    }
   ],
   "source": [
    "for tok in zh_nlp('姚明进入总决赛'):\n",
    "    print(tok)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence splitting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il sentence splitting è eseguito *dopo* la tokenizzazione."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UDPipe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[UDPipe](https://lindat.mff.cuni.cz/services/udpipe/) è un'altra serie di applicazioni e modelli in \"pipeline\" per eseguire analisi linguistica. Nelle prossime lezioni vedremo meglio anche le altre componenti. Per ora concentriamoci sulla tokenizzazione e sul sentence splitting.\n",
    "\n",
    "Un'interfaccia `UDPipe` in Python è disponibile, ma per questo semplice esercizio useremo Python solo per interrogare l'applicazione disponibile sul web. Invieremo la frase da processare via internet al servizio e riceveremo in risposta il testo analizzato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_ita = \"Le chance che la Fed non alzi i tassi di interesse alla prossima riunione di marzo sono al 66%. È quanto emerge dall'andamento degli swap, secondo quanto riporta Cnbc.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = {\n",
    "    'data': sent_ita.encode(),\n",
    "    'model': (None, 'italian-isdt-ud-2.10-220711'),\n",
    "    'tokenizer': (None, ''),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post('http://lindat.mff.cuni.cz/services/udpipe/api/process', files=files)\n",
    "assert response.status_code == 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# generator = UDPipe 2, https://lindat.mff.cuni.cz/services/udpipe\n",
      "# udpipe_model = italian-isdt-ud-2.10-220711\n",
      "# udpipe_model_licence = CC BY-NC-SA\n",
      "# newdoc\n",
      "# newpar\n",
      "# sent_id = 1\n",
      "# text = Le chance che la Fed non alzi i tassi di interesse alla prossima riunione di marzo sono al 66%.\n",
      "1\tLe\t_\t_\t_\t_\t_\t_\t_\t_\n",
      "2\tchance\t_\t_\t_\t_\t_\t_\t_\t_\n",
      "3\tche\t_\t_\t_\t_\t_\t_\t_\t_\n",
      "4\tla\t_\t_\t_\t_\t_\t_\t_\t_\n",
      "5\tFed\t_\t_\t_\t_\t_\t_\t_\t_\n",
      "6\tnon\t_\t_\t_\t_\t_\t_\t_\t_\n",
      "7\talzi\t_\t_\t_\t_\t_\t_\t_\t_\n",
      "8\ti\t_\t_\t_\t_\t_\t_\t_\t_\n",
      "9\ttassi\t_\t_\t_\t_\t_\t_\t_\t_\n",
      "10\tdi\t_\t_\t_\t_\t_\t_\t_\t_\n",
      "11\tinteresse\t_\t_\t_\t_\t_\t_\t_\t_\n",
      "12-13\talla\t_\t_\t_\t_\t_\t_\t_\t_\n",
      "12\ta\t_\t_\t_\t_\t_\t_\t_\t_\n",
      "13\tla\t_\t_\t_\t_\t_\t_\t_\t_\n",
      "14\tprossima\t_\t_\t_\t_\t_\t_\t_\t_\n",
      "15\triunione\t_\t_\t_\t_\t_\t_\t_\t_\n",
      "16\tdi\t_\t_\t_\t_\t_\t_\t_\t_\n",
      "17\tmarzo\t_\t_\t_\t_\t_\t_\t_\t_\n",
      "18\tsono\t_\t_\t_\t_\t_\t_\t_\t_\n",
      "19-20\tal\t_\t_\t_\t_\t_\t_\t_\t_\n",
      "19\ta\t_\t_\t_\t_\t_\t_\t_\t_\n",
      "20\til\t_\t_\t_\t_\t_\t_\t_\t_\n",
      "21\t66\t_\t_\t_\t_\t_\t_\t_\tSpaceAfter=No\n",
      "22\t%\t_\t_\t_\t_\t_\t_\t_\tSpaceAfter=No\n",
      "23\t.\t_\t_\t_\t_\t_\t_\t_\t_\n",
      "\n",
      "# sent_id = 2\n",
      "# text = È quanto emerge dall'andamento degli swap, secondo quanto riporta Cnbc.\n",
      "1\tÈ\t_\t_\t_\t_\t_\t_\t_\t_\n",
      "2\tquanto\t_\t_\t_\t_\t_\t_\t_\t_\n",
      "3\temerge\t_\t_\t_\t_\t_\t_\t_\t_\n",
      "4-5\tdall'\t_\t_\t_\t_\t_\t_\t_\tSpaceAfter=No\n",
      "4\tda\t_\t_\t_\t_\t_\t_\t_\t_\n",
      "5\tl'\t_\t_\t_\t_\t_\t_\t_\t_\n",
      "6\tandamento\t_\t_\t_\t_\t_\t_\t_\t_\n",
      "7-8\tdegli\t_\t_\t_\t_\t_\t_\t_\t_\n",
      "7\tdi\t_\t_\t_\t_\t_\t_\t_\t_\n",
      "8\tgli\t_\t_\t_\t_\t_\t_\t_\t_\n",
      "9\tswap\t_\t_\t_\t_\t_\t_\t_\tSpaceAfter=No\n",
      "10\t,\t_\t_\t_\t_\t_\t_\t_\t_\n",
      "11\tsecondo\t_\t_\t_\t_\t_\t_\t_\t_\n",
      "12\tquanto\t_\t_\t_\t_\t_\t_\t_\t_\n",
      "13\triporta\t_\t_\t_\t_\t_\t_\t_\t_\n",
      "14\tCnbc\t_\t_\t_\t_\t_\t_\t_\tSpaceAfter=No\n",
      "15\t.\t_\t_\t_\t_\t_\t_\t_\tSpaceAfter=No\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response.json()['result'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notare un sacco di cose!\n",
    "1. la pipeline ha fatto *sia* tokenizzazione, *sia* sentence splitting\n",
    "2. il processo di tokenizzazione non è distruttivo (notare `SpaceAfter=No`): posso sempre ricostruire il testo dalla sequenza dei token\n",
    "3. notare il trattamento di *alla* (12-13)\n",
    "4. nella richiesta abbiamo dovuto specificare non tanto la lingua (come in `spacy`), quanto il nome di un **modello** addestrato!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proviamo il cinese!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_zh = '姚明进入总决赛 '\n",
    "\n",
    "files = {\n",
    "    'data': sent_zh.encode(),\n",
    "    'model': (None, 'chinese-gsdsimp-ud-2.6-200830'),\n",
    "    'tokenizer': (None, ''),\n",
    "    'tagger': (None, ''),\n",
    "    'parser': (None, '')\n",
    "}\n",
    "\n",
    "response = requests.post('http://lindat.mff.cuni.cz/services/udpipe/api/process', files=files)\n",
    "assert response.status_code == 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# generator = UDPipe 2, https://lindat.mff.cuni.cz/services/udpipe\n",
      "# udpipe_model = chinese-gsdsimp-ud-2.6-200830\n",
      "# udpipe_model_licence = CC BY-NC-SA\n",
      "# newdoc\n",
      "# newpar\n",
      "# sent_id = 1\n",
      "# text = 姚明进入总决赛\n",
      "1\t姚明\t姚明\tPROPN\tNNP\t_\t2\tnsubj\t_\tSpaceAfter=No\n",
      "2\t进入\t进入\tVERB\tVV\t_\t0\troot\t_\tSpaceAfter=No\n",
      "3\t总\t总\tPART\tPFA\t_\t4\tcase:pref\t_\tSpaceAfter=No\n",
      "4\t决赛\t决赛\tNOUN\tNN\t_\t2\tobj\t_\t_\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response.json()['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP environment (nlp)",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
