{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Un semplice modello linguistico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Costruiamo uno \"N-gram Language Model\" allenandolo sui testi di Sanremo. Partiamo dal modello più semplice e usiamo bigrammi e trigrammi.\n",
    "\n",
    "Un'ottima guida per costruire un semplice modello del genere attraverso l'utilissima libreria [NLTK](https://www.nltk.org/) può essere consultata [qui](https://www.nltk.org/api/nltk.lm.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from nltk.lm import MLE\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "import nltk\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caricare il corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per prima cosa carichiamo il corpus. Usiamo l'ottima libreria [NLTK]() per gestire i molti file in cui il corpus è diviso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = os.path.expanduser('~/Documents/sync/data/Sanremo_testi')\n",
    "os.path.isdir(root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_sanremo = PlaintextCorpusReader(root, r'.*\\.txt')\n",
    "len(corpus_sanremo.fileids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il corpus contiene 244 file.\n",
    "\n",
    "Ora, da ciascuno di loro, estraiamo il testo ed eseguiamo alcune comuni operazioni di preparazione:\n",
    "\n",
    "* eseguiamo il *sentence splitting* e la tokenizzaione (NLTK lo farà per noi)\n",
    "* eliminiamo i segni di punteggiatura\n",
    "* convertiamo tutte le maiuscole in minuscole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = []\n",
    "for s in corpus_sanremo.sents():\n",
    "    toks = [t.lower() for t in s if t.isalnum()]\n",
    "    sents.append(toks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alcune statistiche di base sul nostro corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are {len(sents)} sentences in the corpus; \\nThere are {sum([len(s) for s in sents])} tokens in the corpus')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Addestriamo il nostro modello"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding e bigrammi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`NLTK.lm` mette a nostra disposizione uno strumento molto utile che permette di fare 3 cose molto facilmente:\n",
    "\n",
    "- inserire due token speciali (`<s>` e `</s>`) per l'inizio e fine di frase\n",
    "- calcolare tutti gli $n$-grammi fino ad un massimo dato (es. 3 per unigrammi, bigrammi e trigrammi)\n",
    "- definire il nostro vocabolario (l'insieme di tutte le forme usate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, vocab = padded_everygram_pipeline(3, sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addestriamo i modelli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per il bigram model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_lm = MLE(2)\n",
    "bi_lm.fit(train, vocab)\n",
    "print(bi_lm.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, vocab = padded_everygram_pipeline(3, sents)\n",
    "\n",
    "lm = MLE(3)\n",
    "lm.fit(train, vocab)\n",
    "print(lm.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Esploriamo i modelli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contare i bigrammi (trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.counts[['il', 'mio']].N()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.counts[['un', 'brutto']].most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.counts[['il', 'mio']].most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.counts['amore']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilità"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qual è la probabilità di una parola $W$ data una sequenza? Vediamo alcune probabilità del modello a trigrammi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.score('amore', ['il', 'mio'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ricordatevi che, secondo la LME, la probabilità di un trigramma si calcola secondo la formula\n",
    "\n",
    "$P(amore|il\\;mio) = \\frac{f(\\text{il mio amore})}{f(\\text{il mio })}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gens = lm.generate(10, text_seed=['amore', 'mio'], random_seed=3)\n",
    "print(' '.join(gens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qual è la probabilità di \"mio\" se la parola che precede è \"amore\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qual è la probabilità di \"cuore\" preceduto da \"mio\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.score('cuore', ['il', 'mio'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La freq di \"il mio\" è:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_il_mio = lm.counts[['il', 'mio']].N()\n",
    "print(f_il_mio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La freq di \"il mio amore\" è:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_il_mio_amore = lm.counts[['il', 'mio']]['amore']\n",
    "print(f_il_mio_amore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quali sono le 5 parole più probabili dopo un dato bigramma?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_5_most_probable_words(lm, bigram):\n",
    "    word_probs = {word: lm.score(word, bigram) for word in lm.vocab}\n",
    "    return sorted(word_probs.items(), key=lambda x: x[1], reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_top_5_most_probable_words(lm, ['dammi', 'un'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generare testo!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possiamo usare il nostro modello per generare del testo dato un contesto iniziale (es. un bigramma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.generate(10, text_seed=['il', 'mio'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Un generatore più sofisticato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "detokenize = TreebankWordDetokenizer().detokenize\n",
    "\n",
    "def generate_from_seed(model, seed_words, num_words=10, random_seed=42):\n",
    "    \"\"\"\n",
    "    Generate text starting from a given seed word.\n",
    "    \n",
    "    :param model: An ngram language model from `nltk.lm.model`.\n",
    "    :param seed_words: The initial words to start generation.\n",
    "    :param num_words: Max number of words to generate.\n",
    "    :param random_seed: Seed value for randomness.\n",
    "    :return: A generated sentence as a string.\n",
    "    \"\"\"\n",
    "    content = list(seed_words)  # Start with the seed word\n",
    "    context = tuple(seed_words)  # Context for the bigram model\n",
    "\n",
    "    for _ in range(num_words - 1):  # -1 because seed word is already added\n",
    "        next_word = model.generate(1, text_seed=context, random_seed=random_seed)\n",
    "        if next_word == '</s>':  # Stop at sentence end\n",
    "            break\n",
    "        content.append(next_word)\n",
    "        context = (next_word,)  # Update context\n",
    "\n",
    "    return detokenize(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_from_seed(lm, ['il', 'mio'], num_words=10, random_seed=32)\n",
    "generate_from_seed(lm, ['un', 'bel'], num_words=15, random_seed=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ancora più sofisticato"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aumentiamo la randomizzazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "import random\n",
    "\n",
    "detokenize = TreebankWordDetokenizer().detokenize\n",
    "\n",
    "def generate_from_seed_trigram(model, seed_words, max_length=100, temperature=0.8):\n",
    "    \"\"\"\n",
    "    Generate text from a trigram model with a stochastic selection mechanism to avoid loops.\n",
    "\n",
    "    :param model: A trained trigram language model from `nltk.lm.model`.\n",
    "    :param seed_words: A tuple or list containing exactly two seed words.\n",
    "    :param max_length: Maximum number of words to generate (default: 100).\n",
    "    :param temperature: A parameter that controls randomness (higher = more randomness).\n",
    "    :return: A generated sentence as a string.\n",
    "    \"\"\"\n",
    "    if not isinstance(seed_words, (list, tuple)) or len(seed_words) != 2:\n",
    "        raise ValueError(\"seed_words must be a list or tuple containing exactly two words.\")\n",
    "\n",
    "    content = list(seed_words)  # Start with the two given words\n",
    "    context = tuple(seed_words)  # Initialize context for the trigram model\n",
    "\n",
    "    for _ in range(max_length - 2):  # -2 because we already have two words\n",
    "        possible_next_words = list(model.context_counts(context).keys())\n",
    "        \n",
    "        if not possible_next_words:  # No next word found, stop generation\n",
    "            break\n",
    "\n",
    "        # Get probabilities of next words\n",
    "        probabilities = [model.score(w, context) for w in possible_next_words]\n",
    "\n",
    "        # Apply softmax-like temperature scaling\n",
    "        probabilities = [p ** (1 / temperature) for p in probabilities]\n",
    "        total_prob = sum(probabilities)\n",
    "        probabilities = [p / total_prob for p in probabilities]\n",
    "\n",
    "        # Sample next word based on probabilities\n",
    "        next_word = random.choices(possible_next_words, weights=probabilities, k=1)[0]\n",
    "\n",
    "        if next_word == '</s>':  # Stop at end-of-sentence token\n",
    "            break\n",
    "\n",
    "        content.append(next_word)\n",
    "        context = (context[1], next_word)  # Update context with the last two words\n",
    "\n",
    "    return detokenize(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_from_seed_trigram(lm, \n",
    "                         ['si', 'inghiottì'],\n",
    "                         max_length=50,\n",
    "                         temperature=1.5\n",
    "                         ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendice: vedere le concordanze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.text import Text\n",
    "\n",
    "conc = Text(corpus_sanremo.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conc.concordance(['amore', 'che'], width=100, lines=40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
