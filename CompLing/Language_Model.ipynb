{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Un semplice modello linguistico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Costruiamo uno \"N-gram Language Model\" allenandolo sui testi di Sanremo. Partiamo dal modello più semplice e usiamo bigrammi e trigrammi.\n",
    "\n",
    "Un'ottima guida per costruire un semplice modello del genere attraverso l'utilissima libreria [NLTK](https://www.nltk.org/) può essere consultata [qui](https://www.nltk.org/api/nltk.lm.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from nltk.lm import MLE\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "import nltk\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caricare il corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per prima cosa carichiamo il corpus. Usiamo l'ottima libreria [NLTK]() per gestire i molti file in cui il corpus è diviso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root = os.path.expanduser('~/Documents/sync/data/Sanremo_testi')\n",
    "os.path.isdir(root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "244"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_sanremo = PlaintextCorpusReader(root, r'.*\\.txt')\n",
    "len(corpus_sanremo.fileids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il corpus contiene 244 file.\n",
    "\n",
    "Ora, da ciascuno di loro, estraiamo il testo ed eseguiamo alcune comuni operazioni di preparazione:\n",
    "\n",
    "* eseguiamo il *sentence splitting* e la tokenizzaione (NLTK lo farà per noi)\n",
    "* eliminiamo i segni di punteggiatura\n",
    "* convertiamo tutte le maiuscole in minuscole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = []\n",
    "for s in corpus_sanremo.sents():\n",
    "    toks = [t.lower() for t in s if t.isalnum()]\n",
    "    sents.append(toks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alcune statistiche di base sul nostro corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 9825 sentences in the corpus; \n",
      "There are 552556 tokens in the corpus\n"
     ]
    }
   ],
   "source": [
    "print(f'There are {len(sents)} sentences in the corpus; \\nThere are {sum([len(s) for s in sents])} tokens in the corpus')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Addestriamo il nostro modello"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding e bigrammi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`NLTK.lm` mette a nostra disposizione uno strumento molto utile che permette di fare 3 cose molto facilmente:\n",
    "\n",
    "- inserire due token speciali (`<s>` e `</s>`) per l'inizio e fine di frase\n",
    "- calcolare tutti gli $n$-grammi fino ad un massimo dato (es. 3 per unigrammi, bigrammi e trigrammi)\n",
    "- definire il nostro vocabolario (l'insieme di tutte le forme usate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, vocab = padded_everygram_pipeline(3, sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addestriamo i modelli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per il bigram model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Vocabulary with cutoff=1 unk_label='<UNK>' and 20314 items>\n"
     ]
    }
   ],
   "source": [
    "bi_lm = MLE(2)\n",
    "bi_lm.fit(train, vocab)\n",
    "print(bi_lm.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Vocabulary with cutoff=1 unk_label='<UNK>' and 20314 items>\n"
     ]
    }
   ],
   "source": [
    "train, vocab = padded_everygram_pipeline(3, sents)\n",
    "\n",
    "lm = MLE(3)\n",
    "lm.fit(train, vocab)\n",
    "print(lm.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Esploriamo i modelli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contare i bigrammi (trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "767"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.counts[['il', 'mio']].N()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('voto', 2), ('sogno', 1)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.counts[['un', 'brutto']].most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cuore', 151), ('amore', 53), ('nome', 31), ('mondo', 30), ('destino', 21)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.counts[['il', 'mio']].most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3880"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.counts['amore']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilità"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "che non la smette più e a volte il cuore\n"
     ]
    }
   ],
   "source": [
    "gens = lm.generate(10, text_seed=['amore', 'mio'], random_seed=3)\n",
    "print(' '.join(gens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qual è la probabilità di una parola $W$ data una sequenza? Vediamo alcune probabilità del modello a trigrammi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06910039113428944"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.score('amore', ['il', 'mio'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ricordatevi che, secondo la MLE, la probabilità di un trigramma si calcola secondo la formula\n",
    "\n",
    "$P(amore|il\\;mio) = \\frac{f(\\text{il mio amore})}{f(\\text{il mio })}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qual è la probabilità di \"mio\" se la parola che precede è \"amore\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qual è la probabilità di \"cuore\" preceduto da \"mio\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06910039113428944"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.score('amore', ['il', 'mio'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La freq di \"il mio\" è:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "767\n"
     ]
    }
   ],
   "source": [
    "f_il_mio = lm.counts[['il', 'mio']].N()\n",
    "print(f_il_mio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La freq di \"il mio amore\" è:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53\n"
     ]
    }
   ],
   "source": [
    "f_il_mio_amore = lm.counts[['il', 'mio']]['amore']\n",
    "print(f_il_mio_amore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06910039113428944"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "53 / 767"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quali sono le 5 parole più probabili dopo un dato bigramma?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_5_most_probable_words(lm, bigram):\n",
    "    word_probs = {word: lm.score(word, bigram) for word in lm.vocab}\n",
    "    return sorted(word_probs.items(), key=lambda x: x[1], reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bacio', 0.6229508196721312),\n",
       " ('po', 0.04918032786885246),\n",
       " ('minuto', 0.04918032786885246),\n",
       " ('attimo', 0.03278688524590164),\n",
       " ('grande', 0.03278688524590164)]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_5_most_probable_words(lm, ['dammi', 'un'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generare testo!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possiamo usare il nostro modello per generare del testo dato un contesto iniziale (es. un bigramma). Generare un testo sulla base della probabilità degli eventi si dice **sampling**. Più tecnicamente, **sampling** è il processo di estrarre (campionare) un token dalla distribuzione di probabilità predetta dal modello, dato un certo contesto. Estraiamo (\"a caso\", ma rispettando la distribuzione delle probabilità) un token dal suo contesto precedente. Ad esempio, per fare sampling dalla frase `il gatto dorme sul`, se il nostro modello avesse la seguente distribuzione di probabilità:\n",
    "- `divano`: 0.4\n",
    "- `cuscino`: 0.35\n",
    "- `tavolo`: 0.1\n",
    "\n",
    "\"tireremmo una specie di dado truccato\" (per così dire!) in cui \"divano\" ha il 40% di probabilità di essere scelto, \"cuscino\" il 35%, \"tavolo\" il 10%..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possiamo fare sampling per generare testo a partire da un contesto di partenza. Nel caso del nostro trigram model, il contesto è un bigramma (che può anche comprendere il token speciale `<s>` per l'inizio di frase):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "musica antica nemmeno una risposta e scusa ma non c\n"
     ]
    }
   ],
   "source": [
    "g = lm.generate(10, text_seed=['<s>', 'una'], random_seed=42)\n",
    "print(\" \".join(g))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Un generatore più sofisticato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "detokenize = TreebankWordDetokenizer().detokenize\n",
    "\n",
    "def generate_from_seed(model, seed_words, num_words=10, random_seed=42):\n",
    "    \"\"\"\n",
    "    Generate text starting from a given seed word.\n",
    "    \n",
    "    :param model: An ngram language model from `nltk.lm.model`.\n",
    "    :param seed_words: The initial words to start generation.\n",
    "    :param num_words: Max number of words to generate.\n",
    "    :param random_seed: Seed value for randomness.\n",
    "    :return: A generated sentence as a string.\n",
    "    \"\"\"\n",
    "    content = list(seed_words)  # Start with the seed word\n",
    "    context = tuple(seed_words)  # Context for the bigram model\n",
    "\n",
    "    for _ in range(num_words - 1):  # -1 because seed word is already added\n",
    "        next_word = model.generate(1, text_seed=context, random_seed=random_seed)\n",
    "        if next_word == '</s>':  # Stop at sentence end\n",
    "            break\n",
    "        content.append(next_word)\n",
    "        context = (next_word,)  # Update context\n",
    "\n",
    "    return detokenize(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'il mio amore amore amore amore amore amore amore amore amore amore amore amore amore amore amore amore amore amore amore amore amore amore amore amore amore amore amore amore amore amore amore amore amore amore amore amore amore amore amore amore amore amore amore amore amore amore amore amore amore'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate_from_seed(lm, ['il', 'mio'], num_words=10, random_seed=32)\n",
    "generate_from_seed(lm, ['il', 'mio'], num_words=50, random_seed=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mai',\n",
       " 'pensato',\n",
       " 'di',\n",
       " 'lasciarti',\n",
       " 'vivere',\n",
       " 'ma',\n",
       " 'poi',\n",
       " 'ti',\n",
       " 'fanno',\n",
       " 'sincero']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.generate(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ancora più sofisticato"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aumentiamo la randomizzazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "import random\n",
    "\n",
    "detokenize = TreebankWordDetokenizer().detokenize\n",
    "\n",
    "def generate_from_seed_trigram(model, seed_words, max_length=100, temperature=0.8):\n",
    "    \"\"\"\n",
    "    Generate text from a trigram model with a stochastic selection mechanism to avoid loops.\n",
    "\n",
    "    :param model: A trained trigram language model from `nltk.lm.model`.\n",
    "    :param seed_words: A tuple or list containing exactly two seed words.\n",
    "    :param max_length: Maximum number of words to generate (default: 100).\n",
    "    :param temperature: A parameter that controls randomness (higher = more randomness).\n",
    "    :return: A generated sentence as a string.\n",
    "    \"\"\"\n",
    "    if not isinstance(seed_words, (list, tuple)) or len(seed_words) != 2:\n",
    "        raise ValueError(\"seed_words must be a list or tuple containing exactly two words.\")\n",
    "\n",
    "    content = list(seed_words)  # Start with the two given words\n",
    "    context = tuple(seed_words)  # Initialize context for the trigram model\n",
    "\n",
    "    for _ in range(max_length - 2):  # -2 because we already have two words\n",
    "        possible_next_words = list(model.context_counts(context).keys())\n",
    "        \n",
    "        if not possible_next_words:  # No next word found, stop generation\n",
    "            break\n",
    "\n",
    "        # Get probabilities of next words\n",
    "        probabilities = [model.score(w, context) for w in possible_next_words]\n",
    "\n",
    "        # Apply softmax-like temperature scaling\n",
    "        probabilities = [p ** (1 / temperature) for p in probabilities]\n",
    "        total_prob = sum(probabilities)\n",
    "        probabilities = [p / total_prob for p in probabilities]\n",
    "\n",
    "        # Sample next word based on probabilities\n",
    "        next_word = random.choices(possible_next_words, weights=probabilities, k=1)[0]\n",
    "\n",
    "        if next_word == '</s>':  # Stop at end-of-sentence token\n",
    "            break\n",
    "\n",
    "        content.append(next_word)\n",
    "        context = (context[1], next_word)  # Update context with the last two words\n",
    "\n",
    "    return detokenize(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sei mia e portami via amore amore amore sta zitto non fiatare non franare di più se c è sei qui sei sempre stanco perché tu perseguitavi i cristiani e giustamente lui ti ricorderà il segreto dell amor non potrò scordare mai ma domani partirà se ne vanno e tu\n"
     ]
    }
   ],
   "source": [
    "print(generate_from_seed_trigram(lm, \n",
    "                         ['sei', 'mia'],\n",
    "                         max_length=50,\n",
    "                         temperature=1.3\n",
    "                         ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ## Appendice: vedere le concordanze -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.text import Text\n",
    "\n",
    "conc = Text(corpus_sanremo.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 4 of 4 matches:\n",
      " croce E chi prega sui tappeti Le chiese e le moschee l ’ Imàm e tutti i preti Ingressi separati de\n",
      " croce E chi prega sui tappeti Le chiese e le moschee l ’ Imàm e tutti i preti Ingressi separati de\n",
      " balalajka Lungo i giardini tra le croci e le moschee Il fiume va più nero della sera Oltre la torr\n",
      " croce , chi prega sui tappeti Le chiese e le moschee , gli imam e tutti i preti Ingressi separati \n"
     ]
    }
   ],
   "source": [
    "conc.concordance(['moschee'], width=100, lines=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({})"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.context_counts(('foooffa'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'baciare'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.generate(random_seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating with seen context ('pieno', 'di'):\n",
      "Counter({'poesia': 12, 'vele': 10, 'guai': 8, 'bagliori': 7, 'foschia': 6, 'nutella': 6, 'illusioni': 6, 'pirati': 6, 'cuori': 5, 'domande': 4, 'sensi': 4, 'speranze': 4, 'emozioni': 4, 'brillanti': 4, 'sole': 4, 'promesse': 3, 'volanti': 2, 'noi': 2, 'note': 2, 'malinconia': 1})\n",
      "\n",
      "Generating with OOV context ('tazzina', 'di'):\n",
      "Counter({'me': 13, 'più': 12, 'un': 8, 'te': 5, 'cambiare': 3, 'una': 3, 'chi': 2, 'cosa': 1, 'imparare': 1, 'avere': 1, 'restare': 1, 'abbracciarti': 1, 'cuoio': 1, 'testa': 1, 'amare': 1, 'memoria': 1, 'proibito': 1, 'sfiorarti': 1, 'luce': 1, 'gridare': 1, 'volare': 1, 'conchiglie': 1, 'questa': 1, 'vedere': 1, 'darti': 1, 'che': 1, 'caldi': 1, 'materie': 1, 'camminare': 1, 'scatto': 1, 'notte': 1, 'nuovo': 1, 'sbagliare': 1, 'averti': 1, 'nirvana': 1, 'potertele': 1, 'quel': 1, 'non': 1, 'sera': 1, 'dio': 1, 'vento': 1, 'cantare': 1, 'stanlio': 1, 'voi': 1, 'andarmene': 1, 'perdere': 1, 'tosse': 1, 'decidere': 1, 'credere': 1, 'ogni': 1, 'tarso': 1, 'morire': 1, 'parigi': 1, 'rimanere': 1, 'sparire': 1, 'ragazzi': 1, 'pace': 1, 'lei': 1, 'parlare': 1, 'buttarci': 1, 'domenica': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Known context: \"voglio un\"\n",
    "print(\"Generating with seen context ('pieno', 'di'):\")\n",
    "generated_seen = [lm.generate(1, text_seed=(\"pieno\", \"di\")) for _ in range(100)]\n",
    "seen_counter = Counter(generated_seen)\n",
    "print(seen_counter)\n",
    "\n",
    "# OOV context: \"voglio foooffa\" (never seen in training)\n",
    "print(\"\\nGenerating with OOV context ('tazzina', 'di'):\")\n",
    "generated_oov = [lm.generate(1, text_seed=(\"tazzina\", \"di\")) for _ in range(100)]\n",
    "oov_counter = Counter(generated_oov)\n",
    "print(oov_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unigram counts manually\n",
    "unigram_counts = {word: lm.counts[word] for word in lm.vocab}\n",
    "total_unigrams = sum(unigram_counts.values())\n",
    "\n",
    "# Convert to probabilities\n",
    "unigram_probs = {word: count / total_unigrams for word, count in unigram_counts.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0028858370955097187"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_probs['così']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'così'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.generate(num_words=1, text_seed=['tazzina', 'di'], random_seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'guai': 2, 'poesia': 2, 'emozioni': 1, 'bagliori': 1, 'sole': 1, 'speranze': 1, 'vele': 1, 'cuori': 1, 'pirati': 1, 'foschia': 1, ...})"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.counts[('pieno', 'di')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({})"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_lm.counts[('tazzina',)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pensato'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_lm.generate(num_words=1, text_seed=['tazzina'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_lm.score('di', ['tazza'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_lm.score('di', ['tazzina'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'di': 1, 'con': 1})"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_lm.counts[['tazza']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "844832356"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "29066**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
