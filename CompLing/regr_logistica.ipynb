{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Esempi di regressione logistica nell'analisi del linguaggio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /opt/homebrew/Caskroom/miniforge/base/envs/data/lib/python3.10/site-packages (1.1.1)\n",
      "Requirement already satisfied: pandas in /opt/homebrew/Caskroom/miniforge/base/envs/data/lib/python3.10/site-packages (1.4.0)\n",
      "Requirement already satisfied: matplotlib in /opt/homebrew/Caskroom/miniforge/base/envs/data/lib/python3.10/site-packages (3.8.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/homebrew/Caskroom/miniforge/base/envs/data/lib/python3.10/site-packages (from scikit-learn) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/homebrew/Caskroom/miniforge/base/envs/data/lib/python3.10/site-packages (from scikit-learn) (1.8.1)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /opt/homebrew/Caskroom/miniforge/base/envs/data/lib/python3.10/site-packages (from scikit-learn) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/homebrew/Caskroom/miniforge/base/envs/data/lib/python3.10/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/homebrew/Caskroom/miniforge/base/envs/data/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/Caskroom/miniforge/base/envs/data/lib/python3.10/site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/homebrew/Caskroom/miniforge/base/envs/data/lib/python3.10/site-packages (from matplotlib) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/homebrew/Caskroom/miniforge/base/envs/data/lib/python3.10/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/homebrew/Caskroom/miniforge/base/envs/data/lib/python3.10/site-packages (from matplotlib) (4.28.5)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/homebrew/Caskroom/miniforge/base/envs/data/lib/python3.10/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/Caskroom/miniforge/base/envs/data/lib/python3.10/site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/homebrew/Caskroom/miniforge/base/envs/data/lib/python3.10/site-packages (from matplotlib) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/homebrew/Caskroom/miniforge/base/envs/data/lib/python3.10/site-packages (from matplotlib) (3.0.7)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/Caskroom/miniforge/base/envs/data/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn pandas matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis delle brevi recensioni"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proviamo ad usare Python per allenare un modello di regressione logistica per eseguire un task di **sentiment analysis** analologo a quello che abbiamo usato come esmpio a lezione."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I dati"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prendiamo una piccola sezione del dataset contenuto nel corpus [Multiemo](https://clarin-pl.eu/dspace/handle/11321/798), un corpus multilingue contenente recensioni in 11 lingue in vari domini. Scegliamo i dati relativi al dominio *medico* in lingua italiana.\n",
    "\n",
    "Questo è un esempio del tipo di testi conenuti:\n",
    "\n",
    "\n",
    ">Super dottore e uomo da grande C . Grande esperienza e diagnosi accurate . Grande pazienza per gli anziani . Mi prendo cura della mia vecchia mamma da anni, e dico che siamo molto fortunati ad avere un medico del genere. Non so davvero cosa avremmo fatto se non fosse stato per il medico. Grazie a questo, mia madre è viva. Ogni visita ad uno specialista viene consultata con lui e penso che sia meglio di chiunque altro. Abbiamo una fiducia quasi illimitata in lui. Puoi fare molto di buono per il tuo medico ancora da scrivere. Purtroppo ha molti pazienti, è sovraccarico di lavoro (per questo temo anche per la sua salute) e l'accesso a lui è difficile ma sempre possibile. `__label__meta_plus_m`\n",
    "\n",
    "\n",
    "La classificazione è a 5 categorie: positivi, negativi, neutri, ambigui. Scartiamo tutti i testi classificati come ambigui e neutri"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La prima cosa che dobbiamo fare è trasformare i nostri testi in un vettore di *features* il cui impatto vogliamo misurare.\n",
    "\n",
    "Prendiamo spunto dalla discussione in [Jurafsky e Martin](https://web.stanford.edu/~jurafsky/slp3/) e usiamo queste 6 variabili:\n",
    "\n",
    "- nr. di parole positive\n",
    "- nr. di parole negative\n",
    "- contiene 'polarity shifters' (e.g. no, non, nessuno)? 1 se il testo ne contiene almeno una, altrimenti 0\n",
    "- conto di pronomi di 1a o 2a persona\n",
    "- punto esclamativo? 1 se il testo contiene il punto esclamativo; 0 in caso contrario\n",
    "- log del numero totale di parole"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per effettuare la classificazione in parole positive e negative (e anche per ottenere la lista dei *polarity shifters*) ho utilizzato un [lessico di polarità dell'italiano](https://dspace-clarin-it.ilc.cnr.it/repository/xmlui/handle/20.500.11752/ILC-73) sviluppato dall'ILC-CNR.\n",
    "\n",
    "Questo è il codice che ho utilizzato per trasformare ogni testo in un vettore di feature:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def process_text(txt):\n",
    "    doc = nlp(txt)\n",
    "    positive_words = 0\n",
    "    negative_words = 0\n",
    "    hasShifters = 0\n",
    "    pronouns = 0\n",
    "    hasExclamations = 0\n",
    "    log_nr = math.log(len(doc))\n",
    "\n",
    "    for tok in doc:\n",
    "        pol = polarity_entries.get(tok.lemma_, 'null')\n",
    "        if pol == 'positive':\n",
    "            positive_words += 1\n",
    "        elif pol == 'negative':\n",
    "            negative_words += 1\n",
    "        \n",
    "        if tok.lemma_ in polarity_shifters:\n",
    "            hasShifters = 1\n",
    "\n",
    "        if tok.lemma_ in ['io', 'tu', 'noi', 'voi']:\n",
    "            pronouns += 1\n",
    "        \n",
    "        if tok.lower_ == '!':\n",
    "            hasExclamations = 1\n",
    "    return positive_words, negative_words, hasShifters, pronouns, hasExclamations, log_nr\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possiamo caricare il dataset (il testo delle recensioni non è riprodotto per comodità)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PosWords</th>\n",
       "      <th>NegWords</th>\n",
       "      <th>hasShift</th>\n",
       "      <th>NrPron</th>\n",
       "      <th>hasExcl</th>\n",
       "      <th>logNr</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LineNr.</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.905275</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.941642</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.779123</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.553877</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5.497168</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         PosWords  NegWords  hasShift  NrPron  hasExcl     logNr  Label\n",
       "LineNr.                                                                \n",
       "1              13         2         1       0        0  4.905275      1\n",
       "2               8         2         1       0        0  4.941642      0\n",
       "3              10         5         0       0        0  4.779123      1\n",
       "4              13         3         1       0        0  4.553877      1\n",
       "5              12         4         1       0        1  5.497168      1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/recensioni_feats.tsv', sep='\\t', index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "quante recensioni pos/neg ci sono?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    957\n",
       "1    764\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Label.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alleniamo il modello"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usiamo la libreria `sckitlearn` per creare ed allenare il modello. La funzione per addestrare il modello richiede due argomenti obbligatori:\n",
    "- una matrice che contiene la serie di features; immaginatela come una tabella con tante righe quante sono le nostre osservazioni e tante colonne quante sono le features\n",
    "- un vettore con le classificazioni corrette\n",
    "\n",
    "Cominciamo a creare l'input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['PosWords', 'NegWords', 'hasShift', 'NrPron', 'hasExcl', 'logNr']].to_numpy()\n",
    "y = df.Label.to_numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora creiamo il modello. \"Creare\" in questo caso non vuol dire *allenare* sui dati. Per ora, vuol solamente dire impostare il funzionamento generale del modello, settando alcuni parametri iniziali. La classe di modelli `LogisticRegression` di `sklearn` ha molti parametri iniziali che possono essere regolarati, come si può vedere dalla sua documentazione:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class LogisticRegression in module sklearn.linear_model._logistic:\n",
      "\n",
      "class LogisticRegression(sklearn.linear_model._base.LinearClassifierMixin, sklearn.linear_model._base.SparseCoefMixin, sklearn.base.BaseEstimator)\n",
      " |  LogisticRegression(penalty='l2', *, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
      " |  \n",
      " |  Logistic Regression (aka logit, MaxEnt) classifier.\n",
      " |  \n",
      " |  In the multiclass case, the training algorithm uses the one-vs-rest (OvR)\n",
      " |  scheme if the 'multi_class' option is set to 'ovr', and uses the\n",
      " |  cross-entropy loss if the 'multi_class' option is set to 'multinomial'.\n",
      " |  (Currently the 'multinomial' option is supported only by the 'lbfgs',\n",
      " |  'sag', 'saga' and 'newton-cg' solvers.)\n",
      " |  \n",
      " |  This class implements regularized logistic regression using the\n",
      " |  'liblinear' library, 'newton-cg', 'sag', 'saga' and 'lbfgs' solvers. **Note\n",
      " |  that regularization is applied by default**. It can handle both dense\n",
      " |  and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit\n",
      " |  floats for optimal performance; any other input format will be converted\n",
      " |  (and copied).\n",
      " |  \n",
      " |  The 'newton-cg', 'sag', and 'lbfgs' solvers support only L2 regularization\n",
      " |  with primal formulation, or no regularization. The 'liblinear' solver\n",
      " |  supports both L1 and L2 regularization, with a dual formulation only for\n",
      " |  the L2 penalty. The Elastic-Net regularization is only supported by the\n",
      " |  'saga' solver.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <logistic_regression>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  penalty : {'l1', 'l2', 'elasticnet', 'none'}, default='l2'\n",
      " |      Specify the norm of the penalty:\n",
      " |  \n",
      " |      - `'none'`: no penalty is added;\n",
      " |      - `'l2'`: add a L2 penalty term and it is the default choice;\n",
      " |      - `'l1'`: add a L1 penalty term;\n",
      " |      - `'elasticnet'`: both L1 and L2 penalty terms are added.\n",
      " |  \n",
      " |      .. warning::\n",
      " |         Some penalties may not work with some solvers. See the parameter\n",
      " |         `solver` below, to know the compatibility between the penalty and\n",
      " |         solver.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |         l1 penalty with SAGA solver (allowing 'multinomial' + L1)\n",
      " |  \n",
      " |  dual : bool, default=False\n",
      " |      Dual or primal formulation. Dual formulation is only implemented for\n",
      " |      l2 penalty with liblinear solver. Prefer dual=False when\n",
      " |      n_samples > n_features.\n",
      " |  \n",
      " |  tol : float, default=1e-4\n",
      " |      Tolerance for stopping criteria.\n",
      " |  \n",
      " |  C : float, default=1.0\n",
      " |      Inverse of regularization strength; must be a positive float.\n",
      " |      Like in support vector machines, smaller values specify stronger\n",
      " |      regularization.\n",
      " |  \n",
      " |  fit_intercept : bool, default=True\n",
      " |      Specifies if a constant (a.k.a. bias or intercept) should be\n",
      " |      added to the decision function.\n",
      " |  \n",
      " |  intercept_scaling : float, default=1\n",
      " |      Useful only when the solver 'liblinear' is used\n",
      " |      and self.fit_intercept is set to True. In this case, x becomes\n",
      " |      [x, self.intercept_scaling],\n",
      " |      i.e. a \"synthetic\" feature with constant value equal to\n",
      " |      intercept_scaling is appended to the instance vector.\n",
      " |      The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n",
      " |  \n",
      " |      Note! the synthetic feature weight is subject to l1/l2 regularization\n",
      " |      as all other features.\n",
      " |      To lessen the effect of regularization on synthetic feature weight\n",
      " |      (and therefore on the intercept) intercept_scaling has to be increased.\n",
      " |  \n",
      " |  class_weight : dict or 'balanced', default=None\n",
      " |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      " |      If not given, all classes are supposed to have weight one.\n",
      " |  \n",
      " |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      " |      weights inversely proportional to class frequencies in the input data\n",
      " |      as ``n_samples / (n_classes * np.bincount(y))``.\n",
      " |  \n",
      " |      Note that these weights will be multiplied with sample_weight (passed\n",
      " |      through the fit method) if sample_weight is specified.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *class_weight='balanced'*\n",
      " |  \n",
      " |  random_state : int, RandomState instance, default=None\n",
      " |      Used when ``solver`` == 'sag', 'saga' or 'liblinear' to shuffle the\n",
      " |      data. See :term:`Glossary <random_state>` for details.\n",
      " |  \n",
      " |  solver : {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'},             default='lbfgs'\n",
      " |  \n",
      " |      Algorithm to use in the optimization problem. Default is 'lbfgs'.\n",
      " |      To choose a solver, you might want to consider the following aspects:\n",
      " |  \n",
      " |          - For small datasets, 'liblinear' is a good choice, whereas 'sag'\n",
      " |            and 'saga' are faster for large ones;\n",
      " |          - For multiclass problems, only 'newton-cg', 'sag', 'saga' and\n",
      " |            'lbfgs' handle multinomial loss;\n",
      " |          - 'liblinear' is limited to one-versus-rest schemes.\n",
      " |  \n",
      " |      .. warning::\n",
      " |         The choice of the algorithm depends on the penalty chosen:\n",
      " |         Supported penalties by solver:\n",
      " |  \n",
      " |         - 'newton-cg'   -   ['l2', 'none']\n",
      " |         - 'lbfgs'       -   ['l2', 'none']\n",
      " |         - 'liblinear'   -   ['l1', 'l2']\n",
      " |         - 'sag'         -   ['l2', 'none']\n",
      " |         - 'saga'        -   ['elasticnet', 'l1', 'l2', 'none']\n",
      " |  \n",
      " |      .. note::\n",
      " |         'sag' and 'saga' fast convergence is only guaranteed on\n",
      " |         features with approximately the same scale. You can\n",
      " |         preprocess the data with a scaler from :mod:`sklearn.preprocessing`.\n",
      " |  \n",
      " |      .. seealso::\n",
      " |         Refer to the User Guide for more information regarding\n",
      " |         :class:`LogisticRegression` and more specifically the\n",
      " |         `Table <https://scikit-learn.org/dev/modules/linear_model.html#logistic-regression>`_\n",
      " |         summarazing solver/penalty supports.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         Stochastic Average Gradient descent solver.\n",
      " |      .. versionadded:: 0.19\n",
      " |         SAGA solver.\n",
      " |      .. versionchanged:: 0.22\n",
      " |          The default solver changed from 'liblinear' to 'lbfgs' in 0.22.\n",
      " |  \n",
      " |  max_iter : int, default=100\n",
      " |      Maximum number of iterations taken for the solvers to converge.\n",
      " |  \n",
      " |  multi_class : {'auto', 'ovr', 'multinomial'}, default='auto'\n",
      " |      If the option chosen is 'ovr', then a binary problem is fit for each\n",
      " |      label. For 'multinomial' the loss minimised is the multinomial loss fit\n",
      " |      across the entire probability distribution, *even when the data is\n",
      " |      binary*. 'multinomial' is unavailable when solver='liblinear'.\n",
      " |      'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n",
      " |      and otherwise selects 'multinomial'.\n",
      " |  \n",
      " |      .. versionadded:: 0.18\n",
      " |         Stochastic Average Gradient descent solver for 'multinomial' case.\n",
      " |      .. versionchanged:: 0.22\n",
      " |          Default changed from 'ovr' to 'auto' in 0.22.\n",
      " |  \n",
      " |  verbose : int, default=0\n",
      " |      For the liblinear and lbfgs solvers set verbose to any positive\n",
      " |      number for verbosity.\n",
      " |  \n",
      " |  warm_start : bool, default=False\n",
      " |      When set to True, reuse the solution of the previous call to fit as\n",
      " |      initialization, otherwise, just erase the previous solution.\n",
      " |      Useless for liblinear solver. See :term:`the Glossary <warm_start>`.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.\n",
      " |  \n",
      " |  n_jobs : int, default=None\n",
      " |      Number of CPU cores used when parallelizing over classes if\n",
      " |      multi_class='ovr'\". This parameter is ignored when the ``solver`` is\n",
      " |      set to 'liblinear' regardless of whether 'multi_class' is specified or\n",
      " |      not. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
      " |      context. ``-1`` means using all processors.\n",
      " |      See :term:`Glossary <n_jobs>` for more details.\n",
      " |  \n",
      " |  l1_ratio : float, default=None\n",
      " |      The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only\n",
      " |      used if ``penalty='elasticnet'``. Setting ``l1_ratio=0`` is equivalent\n",
      " |      to using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent\n",
      " |      to using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a\n",
      " |      combination of L1 and L2.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  \n",
      " |  classes_ : ndarray of shape (n_classes, )\n",
      " |      A list of class labels known to the classifier.\n",
      " |  \n",
      " |  coef_ : ndarray of shape (1, n_features) or (n_classes, n_features)\n",
      " |      Coefficient of the features in the decision function.\n",
      " |  \n",
      " |      `coef_` is of shape (1, n_features) when the given problem is binary.\n",
      " |      In particular, when `multi_class='multinomial'`, `coef_` corresponds\n",
      " |      to outcome 1 (True) and `-coef_` corresponds to outcome 0 (False).\n",
      " |  \n",
      " |  intercept_ : ndarray of shape (1,) or (n_classes,)\n",
      " |      Intercept (a.k.a. bias) added to the decision function.\n",
      " |  \n",
      " |      If `fit_intercept` is set to False, the intercept is set to zero.\n",
      " |      `intercept_` is of shape (1,) when the given problem is binary.\n",
      " |      In particular, when `multi_class='multinomial'`, `intercept_`\n",
      " |      corresponds to outcome 1 (True) and `-intercept_` corresponds to\n",
      " |      outcome 0 (False).\n",
      " |  \n",
      " |  n_features_in_ : int\n",
      " |      Number of features seen during :term:`fit`.\n",
      " |  \n",
      " |      .. versionadded:: 0.24\n",
      " |  \n",
      " |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      " |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      " |      has feature names that are all strings.\n",
      " |  \n",
      " |      .. versionadded:: 1.0\n",
      " |  \n",
      " |  n_iter_ : ndarray of shape (n_classes,) or (1, )\n",
      " |      Actual number of iterations for all classes. If binary or multinomial,\n",
      " |      it returns only 1 element. For liblinear solver, only the maximum\n",
      " |      number of iteration across all classes is given.\n",
      " |  \n",
      " |      .. versionchanged:: 0.20\n",
      " |  \n",
      " |          In SciPy <= 1.0.0 the number of lbfgs iterations may exceed\n",
      " |          ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  SGDClassifier : Incrementally trained logistic regression (when given\n",
      " |      the parameter ``loss=\"log\"``).\n",
      " |  LogisticRegressionCV : Logistic regression with built-in cross validation.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The underlying C implementation uses a random number generator to\n",
      " |  select features when fitting the model. It is thus not uncommon,\n",
      " |  to have slightly different results for the same input data. If\n",
      " |  that happens, try with a smaller tol parameter.\n",
      " |  \n",
      " |  Predict output may not match that of standalone liblinear in certain\n",
      " |  cases. See :ref:`differences from liblinear <liblinear_differences>`\n",
      " |  in the narrative documentation.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  \n",
      " |  L-BFGS-B -- Software for Large-scale Bound-constrained Optimization\n",
      " |      Ciyou Zhu, Richard Byrd, Jorge Nocedal and Jose Luis Morales.\n",
      " |      http://users.iems.northwestern.edu/~nocedal/lbfgsb.html\n",
      " |  \n",
      " |  LIBLINEAR -- A Library for Large Linear Classification\n",
      " |      https://www.csie.ntu.edu.tw/~cjlin/liblinear/\n",
      " |  \n",
      " |  SAG -- Mark Schmidt, Nicolas Le Roux, and Francis Bach\n",
      " |      Minimizing Finite Sums with the Stochastic Average Gradient\n",
      " |      https://hal.inria.fr/hal-00860051/document\n",
      " |  \n",
      " |  SAGA -- Defazio, A., Bach F. & Lacoste-Julien S. (2014).\n",
      " |          :arxiv:`\"SAGA: A Fast Incremental Gradient Method With Support\n",
      " |          for Non-Strongly Convex Composite Objectives\" <1407.0202>`\n",
      " |  \n",
      " |  Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent\n",
      " |      methods for logistic regression and maximum entropy models.\n",
      " |      Machine Learning 85(1-2):41-75.\n",
      " |      https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.datasets import load_iris\n",
      " |  >>> from sklearn.linear_model import LogisticRegression\n",
      " |  >>> X, y = load_iris(return_X_y=True)\n",
      " |  >>> clf = LogisticRegression(random_state=0).fit(X, y)\n",
      " |  >>> clf.predict(X[:2, :])\n",
      " |  array([0, 0])\n",
      " |  >>> clf.predict_proba(X[:2, :])\n",
      " |  array([[9.8...e-01, 1.8...e-02, 1.4...e-08],\n",
      " |         [9.7...e-01, 2.8...e-02, ...e-08]])\n",
      " |  >>> clf.score(X, y)\n",
      " |  0.97...\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      LogisticRegression\n",
      " |      sklearn.linear_model._base.LinearClassifierMixin\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      sklearn.linear_model._base.SparseCoefMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, penalty='l2', *, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Fit the model according to the given training data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Training vector, where `n_samples` is the number of samples and\n",
      " |          `n_features` is the number of features.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          Target vector relative to X.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,) default=None\n",
      " |          Array of weights that are assigned to individual samples.\n",
      " |          If not provided, then each sample is given unit weight.\n",
      " |      \n",
      " |          .. versionadded:: 0.17\n",
      " |             *sample_weight* support to LogisticRegression.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |          Fitted estimator.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The SAGA solver supports both float64 and float32 bit arrays.\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Predict logarithm of probability estimates.\n",
      " |      \n",
      " |      The returned estimates for all classes are ordered by the\n",
      " |      label of classes.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Vector to be scored, where `n_samples` is the number of samples and\n",
      " |          `n_features` is the number of features.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      T : array-like of shape (n_samples, n_classes)\n",
      " |          Returns the log-probability of the sample for each class in the\n",
      " |          model, where classes are ordered as they are in ``self.classes_``.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Probability estimates.\n",
      " |      \n",
      " |      The returned estimates for all classes are ordered by the\n",
      " |      label of classes.\n",
      " |      \n",
      " |      For a multi_class problem, if multi_class is set to be \"multinomial\"\n",
      " |      the softmax function is used to find the predicted probability of\n",
      " |      each class.\n",
      " |      Else use a one-vs-rest approach, i.e calculate the probability\n",
      " |      of each class assuming it to be positive using the logistic function.\n",
      " |      and normalize these values across all the classes.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Vector to be scored, where `n_samples` is the number of samples and\n",
      " |          `n_features` is the number of features.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      T : array-like of shape (n_samples, n_classes)\n",
      " |          Returns the probability of the sample for each class in the model,\n",
      " |          where classes are ordered as they are in ``self.classes_``.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.linear_model._base.LinearClassifierMixin:\n",
      " |  \n",
      " |  decision_function(self, X)\n",
      " |      Predict confidence scores for samples.\n",
      " |      \n",
      " |      The confidence score for a sample is proportional to the signed\n",
      " |      distance of that sample to the hyperplane.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The data matrix for which we want to get the confidence scores.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      scores : ndarray of shape (n_samples,) or (n_samples, n_classes)\n",
      " |          Confidence scores per `(n_samples, n_classes)` combination. In the\n",
      " |          binary case, confidence score for `self.classes_[1]` where >0 means\n",
      " |          this class would be predicted.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict class labels for samples in X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The data matrix for which we want to get the predictions.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y_pred : ndarray of shape (n_samples,)\n",
      " |          Vector containing the class labels for each sample.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.linear_model._base.SparseCoefMixin:\n",
      " |  \n",
      " |  densify(self)\n",
      " |      Convert coefficient matrix to dense array format.\n",
      " |      \n",
      " |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n",
      " |      default format of ``coef_`` and is required for fitting, so calling\n",
      " |      this method is only required on models that have previously been\n",
      " |      sparsified; otherwise, it is a no-op.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |          Fitted estimator.\n",
      " |  \n",
      " |  sparsify(self)\n",
      " |      Convert coefficient matrix to sparse format.\n",
      " |      \n",
      " |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n",
      " |      L1-regularized models can be much more memory- and storage-efficient\n",
      " |      than the usual numpy.ndarray representation.\n",
      " |      \n",
      " |      The ``intercept_`` member is not converted.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |          Fitted estimator.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n",
      " |      this may actually *increase* memory usage, so use this method with\n",
      " |      care. A rule of thumb is that the number of zero elements, which can\n",
      " |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n",
      " |      to provide significant benefits.\n",
      " |      \n",
      " |      After calling this method, further fitting with the partial_fit\n",
      " |      method (if any) will not work until you call densify.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(LogisticRegression)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noi lavoriamo con i valori di default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(solver='liblinear', random_state=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora possiamo addestrare (\"fare il fit\") dei dati!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(random_state=0, solver=&#x27;liblinear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(random_state=0, solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(random_state=0, solver='liblinear')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ecco fatto! Possiamo vedere quali sono i pesi e quale è l'intercetto calcolato dal modello?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.51674717])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.08800761, -0.16689316, -1.32284647, -0.03628668,  0.31080057,\n",
       "        -0.60220009]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coef_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possiamo vedere le probabilità predette sui dati di training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.48743856, 0.51256144],\n",
       "       [0.60149119, 0.39850881],\n",
       "       [0.33528692, 0.66471308],\n",
       "       [0.47627408, 0.52372592],\n",
       "       [0.60280713, 0.39719287],\n",
       "       [0.51425592, 0.48574408],\n",
       "       [0.6244055 , 0.3755945 ],\n",
       "       [0.32758341, 0.67241659],\n",
       "       [0.75234066, 0.24765934],\n",
       "       [0.84127996, 0.15872004]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_proba(X)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 1, 0, 0, 0, 1, 0, 0])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X)[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valutazione"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Riusiamo I dati di training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Facile, no? Adesso, però, è necessario **valutare** il nostro modello, ovvero sapere quale capacità predittiva abbia il nostro modello! Come facciamo?\n",
    "\n",
    "Una risposta semplice semplice è quella di testare il nostro modello sugli stessi dati su cui si è addestrato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6717024985473562"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uuuh... il nostro modello ha classificato correttamente solo il 67% dei testi di training... numeri non esattamente entusiasmanti!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questo numero, peraltro non ci dice molto. Dove sbaglia il nostro classificatore? Quanti `positivi` classificati come `negativi` e viceversa abbiamo?\n",
    "\n",
    "Per avere questo dato possiamo usare una cosiddetta `matrice di confusione` (confusion matrix), ovvero una tabella di errori di predizioni che ci restituisca il numero di:\n",
    "- 0 classificati correttamente (veri negativi)\n",
    "- 0 classificati erroneamente (falsi negativi)\n",
    "- 1 classificati correttamente (veri positivi)\n",
    "- 1 classificati erroneamente (falsi positivi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[788, 169],\n",
       "       [396, 368]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y, model.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix(y, model.predict(X))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "ax.imshow(cm)\n",
    "ax.xaxis.set(ticks=(0, 1), ticklabels=('Predicted 0s', 'Predicted 1s'))\n",
    "ax.yaxis.set(ticks=(0, 1), ticklabels=('Actual 0s', 'Actual 1s'))\n",
    "ax.set_ylim(1.5, -0.5)\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        ax.text(j, i, cm[i, j], ha='center', va='center', color='red')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.82      0.74       957\n",
      "           1       0.69      0.48      0.57       764\n",
      "\n",
      "    accuracy                           0.67      1721\n",
      "   macro avg       0.68      0.65      0.65      1721\n",
      "weighted avg       0.67      0.67      0.66      1721\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y, model.predict(X)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Usare un test set"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il corpus Multiemo, però, suddivide i dati in 3 parti: train, dev, test. La cosa più corretta da fare è usare i dati di test per valutare le performance del nostro modello. I dati sono già stati organizzati in features e label. Carichiamoli e vediamo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PosWords</th>\n",
       "      <th>NegWords</th>\n",
       "      <th>hasShift</th>\n",
       "      <th>NrPron</th>\n",
       "      <th>hasExcl</th>\n",
       "      <th>logNr</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LineNr.</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.532599</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.859812</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.814131</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.951244</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.099866</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         PosWords  NegWords  hasShift  NrPron  hasExcl     logNr  Label\n",
       "LineNr.                                                                \n",
       "1               5         4         1       0        0  4.532599      0\n",
       "2               8         3         1       0        0  4.859812      0\n",
       "3              35        14         1       0        0  5.814131      0\n",
       "4               1         0         0       0        0  3.951244      1\n",
       "5              16         7         1       0        0  5.099866      1"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv('../data/recensioni_feats_TEST.tsv', sep='\\t', index_col=0)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = df_test[['PosWords', 'NegWords', 'hasShift', 'NrPron', 'hasExcl', 'logNr']].to_numpy()\n",
    "y_test = df_test.Label.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       133\n",
      "           1       0.42      0.98      0.58        97\n",
      "\n",
      "    accuracy                           0.41       230\n",
      "   macro avg       0.21      0.49      0.29       230\n",
      "weighted avg       0.18      0.41      0.25       230\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, model.predict(X_test)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vediamo ancora la matrice di confusione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, model.predict(X_test))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "ax.imshow(cm)\n",
    "ax.xaxis.set(ticks=(0, 1), ticklabels=('Predicted 0s', 'Predicted 1s'))\n",
    "ax.yaxis.set(ticks=(0, 1), ticklabels=('Actual 0s', 'Actual 1s'))\n",
    "ax.set_ylim(1.5, -0.5)\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        ax.text(j, i, cm[i, j], ha='center', va='center', color='red')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosa succederebbe se predicessimo sempre la classe più frequente (ovvero: positivo, 0)? Azzeccheremmo comunque un buon numero di previsioni. Otterremmo, cioè, una accuratezza del 58%. È questo il numero che ci dà una stima più realistica del miglioramento approtato dal nostro modello."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      1.00      0.73       133\n",
      "           1       0.00      0.00      0.00        97\n",
      "\n",
      "    accuracy                           0.58       230\n",
      "   macro avg       0.29      0.50      0.37       230\n",
      "weighted avg       0.33      0.58      0.42       230\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/data/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/data/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/data/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,  [0] * len(y_test) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qui possiamo chiedere al modello di visualizzare le probabilità delle prime 10 osservazioni del test. Per ogni riga, il primo numero è la probabilità della classe 0, il secondo quella della classe 1.\n",
    "\n",
    "Ricordatevi che abbiamo posto la soglia a 0.5: la classe che supera la soglia è quella assegnata dal nostro classificatore. Ad es., nel primo caso 0 ha una probabilità uguale a 0.68; per il nostro classificatore il primo testo sarà positivo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.68204065, 0.31795935],\n",
       "       [0.62931923, 0.37068077],\n",
       "       [0.63728677, 0.36271323],\n",
       "       [0.22700238, 0.77299762],\n",
       "       [0.65415864, 0.34584136],\n",
       "       [0.63292361, 0.36707639],\n",
       "       [0.48275403, 0.51724597],\n",
       "       [0.51694238, 0.48305762],\n",
       "       [0.49653384, 0.50346616],\n",
       "       [0.72552642, 0.27447358]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_proba(X_test)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qui possiamo chiedere di darci in output le label che corrispondono alla probabilità:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 0, 0, 1, 0, 1, 0])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O addirittura possiamo stampare `positivo` e `negativo` come etichetta corrispondente a 0 e 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positivo\n",
      "positivo\n",
      "positivo\n",
      "negativo\n",
      "positivo\n",
      "positivo\n",
      "negativo\n",
      "positivo\n",
      "negativo\n",
      "positivo\n"
     ]
    }
   ],
   "source": [
    "for res in model.predict(X_test)[:10]:\n",
    "    if res == 0:\n",
    "        print(\"positivo\")\n",
    "    else:\n",
    "        print(\"negativo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Che cos'è il **bias** $b$ che osserviamo nella proprietà del modello `model.intercept_` e che compare nella formula:\n",
    "\n",
    "$z = \\vec{w} \\cdot \\vec{x} + b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ricordiamo che $b$, in questa formula, equivale a:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.51674717])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se avessimo un testo $x$ rappresentato da un vettore contenente solo una sequenza di 0 (ovvero: se tutte le feature del nostro testo fossero uguali a 0) otterremmo queste probabilità"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02883946, 0.97116054]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_proba([[0] * 6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ricordate che le probabilità (meglio, la probabilità $p$ della classe 1, dato che la probabilità della classe 0 è semplicemente 1-p) sono calcolate applicando la funzione logistica a $z$.\n",
    "\n",
    "Dunque, che risultato otterremo se applicassimo una funzione ci permette di passare dalla probabilità della classe 1 quando tutte le features sono 0 al nostro coefficiente $z$?\n",
    "\n",
    "La funzione che ci permette di passare dalle probabilità ai log-odds ($z$) è la cosidetta funzione *logit*:\n",
    "\n",
    "$z = \\log \\left(\\frac{p}{1-p}\\right)$\n",
    "\n",
    "Questo è il codice python che serve a calcolarla:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def logit(x):\n",
    "    # le prima 2 righe di codice sono un buon modo per assicurarci che il valore passato \n",
    "    # sia una probabilità valida (tra 0 e 1)\n",
    "    if x <= 0 or x >= 1:\n",
    "        raise ValueError(\"Probability must be between 0 and 1, exclusive.\")\n",
    "    \n",
    "    return math.log(x / (1 - x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proviamola con la probabilità della classe 1 quando tutti i valori delle feature sono 0 (ovvero $p=0.97116054$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.5167472010845353"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit(0.97116054)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il nostro bias è:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.51674717])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il bias non è nient'altro che il valore di $z$ quando il prodotto tra le feature ($\\vec{x}$) e i pesi ($\\vec{w}$) risulta 0\n",
    "\n",
    "*CVD*!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un aspetto utile della regressione logistica è che possiamo usare i pesi ($\\vec{w}$) per farci un'idea delle variabili che influiscono di più sulla classificazione: l'idea (molto grossolana) è che più alto è il peso, più alta sarà l'influenza della feature in questione.\n",
    "\n",
    "Possiamo (informalmente) utilizzare [queste formule](https://sefiks.com/2021/01/06/feature-importance-in-logistic-regression/) per avere un'idea dell'importanza relativa delle diverse features all'interno dell'equazione del modello. Ma una lettura attenta di un buon manuale di statistica per linguisti, come quello di [Gries](https://www.degruyter.com/document/doi/10.1515/9783110718256/html) citato più sotto. è fondamentale per chi volesse utilizzare i modelli lineari più seriamente per lo studio linguistico!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_importance = pd.DataFrame(['PosWords', 'NegWords', 'hasShift', 'NrPron', 'hasExcl', 'logNr'], columns = [\"feature\"])\n",
    "feature_importance[\"importance\"] = pow(math.e, model.coef_[0])\n",
    "feature_importance = feature_importance.sort_values(by = [\"importance\"], ascending=False)\n",
    "ax = feature_importance.plot.barh(x='feature', y='importance')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Studio Linguistico"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il libro *Statistics for Linguistics with R: A Practical Introduction* ([vedi qui](https://www.degruyter.com/document/doi/10.1515/9783110718256/html)) di S. Gries contiene una dettagliata e splendida spiegazione dell'uso della regressione logistica (insieme ad altri modelli) per lo studio linguistico.\n",
    "\n",
    "Tra i vari dataset di esempio, ce n'è uno molto interessante (usato da Gries per illustrare la regressione logistica in contesto di studio quantitativo) relativo al posizionamento delle subordinate temporali e causali rispetto alla principale in un corpus misto inglese e tedesco. È riprodotto qui (dalla seconda ed. che io possiedo) nel file `data/clauseorder.csv`. \n",
    "\n",
    "Carichiamolo e diamogli un'occhiata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CASE</th>\n",
       "      <th>ORDER</th>\n",
       "      <th>SUBORDTYPE</th>\n",
       "      <th>LEN_MC</th>\n",
       "      <th>LEN_SC</th>\n",
       "      <th>LENGTH_DIFF</th>\n",
       "      <th>CONJ</th>\n",
       "      <th>MORETHAN2CL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4777</td>\n",
       "      <td>sc-mc</td>\n",
       "      <td>temp</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>-6</td>\n",
       "      <td>als/when</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1698</td>\n",
       "      <td>mc-sc</td>\n",
       "      <td>temp</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>als/when</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>953</td>\n",
       "      <td>sc-mc</td>\n",
       "      <td>temp</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>als/when</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1681</td>\n",
       "      <td>mc-sc</td>\n",
       "      <td>temp</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>-9</td>\n",
       "      <td>als/when</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4055</td>\n",
       "      <td>sc-mc</td>\n",
       "      <td>temp</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>als/when</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CASE  ORDER SUBORDTYPE  LEN_MC  LEN_SC  LENGTH_DIFF      CONJ MORETHAN2CL\n",
       "0  4777  sc-mc       temp       4      10           -6  als/when          no\n",
       "1  1698  mc-sc       temp       7       6            1  als/when          no\n",
       "2   953  sc-mc       temp      12       7            5  als/when         yes\n",
       "3  1681  mc-sc       temp       6      15           -9  als/when          no\n",
       "4  4055  sc-mc       temp       9       5            4  als/when         yes"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/clauseorders.csv', sep='\\t')\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quello che noi vogliamo predire è l'ordine delle proposizioni, che qui è registrato nella colonna `ORDER`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mc-sc    275\n",
       "sc-mc    128\n",
       "Name: ORDER, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.ORDER.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = ['SUBORDTYPE', 'LEN_MC', 'LEN_SC', 'LENGTH_DIFF',\n",
    "       'CONJ', 'MORETHAN2CL']\n",
    "to_factorize = ['SUBORDTYPE', 'CONJ', 'MORETHAN2CL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CASE</th>\n",
       "      <th>ORDER</th>\n",
       "      <th>SUBORDTYPE</th>\n",
       "      <th>LEN_MC</th>\n",
       "      <th>LEN_SC</th>\n",
       "      <th>LENGTH_DIFF</th>\n",
       "      <th>CONJ</th>\n",
       "      <th>MORETHAN2CL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4777</td>\n",
       "      <td>sc-mc</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>-6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1698</td>\n",
       "      <td>mc-sc</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>953</td>\n",
       "      <td>sc-mc</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1681</td>\n",
       "      <td>mc-sc</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>-9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4055</td>\n",
       "      <td>sc-mc</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CASE  ORDER  SUBORDTYPE  LEN_MC  LEN_SC  LENGTH_DIFF  CONJ  MORETHAN2CL\n",
       "0  4777  sc-mc           0       4      10           -6     0            0\n",
       "1  1698  mc-sc           0       7       6            1     0            0\n",
       "2   953  sc-mc           0      12       7            5     0            1\n",
       "3  1681  mc-sc           0       6      15           -9     0            0\n",
       "4  4055  sc-mc           0       9       5            4     0            1"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for col in to_factorize:\n",
    "    df[col] = df[col].factorize()[0]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qui *non* ripeteremo le analisi molto sofisticate che Gries discute nel suo libro. Ci limitiamo a giocare un po' con un modello creato con `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trasformiamo le due categorie in 0 e 1: sc-mc diventa 0\n",
    "y = df.ORDER.factorize()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(solver='lbfgs', penalty='none', random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(penalty=&#x27;none&#x27;, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(penalty=&#x27;none&#x27;, random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(penalty='none', random_state=0)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df[feats].values\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7667493796526055"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.62135434, -0.01808965,  0.00706139, -0.02515104,  0.06792462,\n",
       "        -0.76710495]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coef_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possiamo (informalmente) utilizzare [queste formule](https://sefiks.com/2021/01/06/feature-importance-in-logistic-regression/) per avere un'idea dell'importanza relativa delle diverse features all'interno dell'equazione del modello. Ma una lettura attenta di Gries è fondamentale per chi volesse utilizzare i modelli lineari più seriamente per lo studio linguistico!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_importance = pd.DataFrame(feats, columns = [\"feature\"])\n",
    "feature_importance[\"importance\"] = pow(math.e, model.coef_[0])\n",
    "feature_importance = feature_importance.sort_values(by = [\"importance\"], ascending=False)\n",
    "ax = feature_importance.plot.barh(x='feature', y='importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SUBORDTYPE</td>\n",
       "      <td>13.754339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CONJ</td>\n",
       "      <td>1.070285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LEN_SC</td>\n",
       "      <td>1.007086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LEN_MC</td>\n",
       "      <td>0.982073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LENGTH_DIFF</td>\n",
       "      <td>0.975163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MORETHAN2CL</td>\n",
       "      <td>0.464355</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       feature  importance\n",
       "0   SUBORDTYPE   13.754339\n",
       "4         CONJ    1.070285\n",
       "2       LEN_SC    1.007086\n",
       "1       LEN_MC    0.982073\n",
       "3  LENGTH_DIFF    0.975163\n",
       "5  MORETHAN2CL    0.464355"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
