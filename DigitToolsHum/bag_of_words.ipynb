{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Bag-of-words model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's get a text to play with. We will use NLTK's tokenizers to play around with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/princess.txt') as f:\n",
    "    txt = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need the tokenizers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import nltk.tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "\n",
    "\n",
    "# download the most recent punkt package\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('treebank', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60075"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks = tt.tokenize(txt)\n",
    "len(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Princess',\n",
       " 'and',\n",
       " 'the',\n",
       " 'Goblin',\n",
       " 'George',\n",
       " 'MacDonald',\n",
       " 'CHAPTER',\n",
       " 'I.',\n",
       " 'WHY',\n",
       " 'THE',\n",
       " 'PRINCESS',\n",
       " 'HAS',\n",
       " 'A',\n",
       " 'STORY',\n",
       " 'ABOUT',\n",
       " 'HER',\n",
       " 'THERE',\n",
       " 'was',\n",
       " 'once',\n",
       " 'a',\n",
       " 'little',\n",
       " 'princess',\n",
       " 'who',\n",
       " '--',\n",
       " \"''\",\n",
       " '_But',\n",
       " ',',\n",
       " 'Mr.',\n",
       " 'Author',\n",
       " ',',\n",
       " 'why',\n",
       " 'do',\n",
       " 'you',\n",
       " 'always',\n",
       " 'write',\n",
       " 'about',\n",
       " 'princesses',\n",
       " '?',\n",
       " '_',\n",
       " \"''\",\n",
       " \"''\",\n",
       " '_Because',\n",
       " 'every',\n",
       " 'little',\n",
       " 'girl',\n",
       " 'is',\n",
       " 'a',\n",
       " 'princess._',\n",
       " \"''\",\n",
       " \"''\",\n",
       " '_You',\n",
       " 'will',\n",
       " 'make',\n",
       " 'them',\n",
       " 'vain',\n",
       " 'if',\n",
       " 'you',\n",
       " 'tell',\n",
       " 'them',\n",
       " 'that._',\n",
       " \"''\",\n",
       " \"''\",\n",
       " '_Not',\n",
       " 'if',\n",
       " 'they',\n",
       " 'understand',\n",
       " 'what',\n",
       " 'I',\n",
       " 'mean._',\n",
       " \"''\",\n",
       " \"''\",\n",
       " '_Then',\n",
       " 'what',\n",
       " 'do',\n",
       " 'you',\n",
       " 'mean',\n",
       " '?',\n",
       " '_',\n",
       " \"''\",\n",
       " \"''\",\n",
       " '_What_',\n",
       " 'do',\n",
       " 'you',\n",
       " '_mean',\n",
       " 'by',\n",
       " 'a',\n",
       " 'princess',\n",
       " '?',\n",
       " '_',\n",
       " \"''\",\n",
       " \"''\",\n",
       " '_The',\n",
       " 'daughter',\n",
       " 'of',\n",
       " 'a',\n",
       " 'king._',\n",
       " \"''\",\n",
       " \"''\",\n",
       " '_Very']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "toks2 = word_tokenize(txt, language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Princess',\n",
       " 'and',\n",
       " 'the',\n",
       " 'Goblin',\n",
       " 'George',\n",
       " 'MacDonald',\n",
       " 'CHAPTER',\n",
       " 'I',\n",
       " '.',\n",
       " 'WHY',\n",
       " 'THE',\n",
       " 'PRINCESS',\n",
       " 'HAS',\n",
       " 'A',\n",
       " 'STORY',\n",
       " 'ABOUT',\n",
       " 'HER',\n",
       " 'THERE',\n",
       " 'was',\n",
       " 'once',\n",
       " 'a',\n",
       " 'little',\n",
       " 'princess',\n",
       " 'who',\n",
       " '--',\n",
       " \"''\",\n",
       " '_But',\n",
       " ',',\n",
       " 'Mr',\n",
       " '.',\n",
       " 'Author',\n",
       " ',',\n",
       " 'why',\n",
       " 'do',\n",
       " 'you',\n",
       " 'always',\n",
       " 'write',\n",
       " 'about',\n",
       " 'princesses',\n",
       " '?',\n",
       " '_',\n",
       " \"''\",\n",
       " \"''\",\n",
       " '_Because',\n",
       " 'every',\n",
       " 'little',\n",
       " 'girl',\n",
       " 'is',\n",
       " 'a',\n",
       " 'princess._',\n",
       " \"''\",\n",
       " \"''\",\n",
       " '_You',\n",
       " 'will',\n",
       " 'make',\n",
       " 'them',\n",
       " 'vain',\n",
       " 'if',\n",
       " 'you',\n",
       " 'tell',\n",
       " 'them',\n",
       " 'that._',\n",
       " \"''\",\n",
       " \"''\",\n",
       " '_Not',\n",
       " 'if',\n",
       " 'they',\n",
       " 'understand',\n",
       " 'what',\n",
       " 'I',\n",
       " 'mean._',\n",
       " \"''\",\n",
       " \"''\",\n",
       " '_Then',\n",
       " 'what',\n",
       " 'do',\n",
       " 'you',\n",
       " 'mean',\n",
       " '?',\n",
       " '_',\n",
       " \"''\",\n",
       " \"''\",\n",
       " '_What_',\n",
       " 'do',\n",
       " 'you',\n",
       " '_mean',\n",
       " 'by',\n",
       " 'a',\n",
       " 'princess',\n",
       " '?',\n",
       " '_',\n",
       " \"''\",\n",
       " \"''\",\n",
       " '_The',\n",
       " 'daughter',\n",
       " 'of',\n",
       " 'a',\n",
       " 'king._',\n",
       " \"''\"]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks2[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62700"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(toks2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = '''Good muffins cost $3.88\\nin New York. Please buy me two of them.\\n\\nThanks.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_tokenize(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tt.tokenize(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good Good\n",
      "muffins muffins\n",
      "cost cost\n",
      "$ $\n",
      "3.88 3.88\n",
      "in in\n",
      "New New\n",
      "York York.\n",
      ". Please\n",
      "Please buy\n",
      "buy me\n",
      "me two\n",
      "two of\n",
      "of them.\n",
      "them Thanks\n",
      ". .\n"
     ]
    }
   ],
   "source": [
    "for p,t in zip(word_tokenize(s), tt.tokenize(s)):\n",
    "    print(p,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " \"'ll\",\n",
       " 'tell',\n",
       " 'you',\n",
       " 'all',\n",
       " 'that',\n",
       " 'I',\n",
       " \"'ve\",\n",
       " 'told',\n",
       " 'to',\n",
       " 'Jenny',\n",
       " 'about',\n",
       " 'Sam',\n",
       " \"'s\",\n",
       " 'book',\n",
       " '.',\n",
       " 'You',\n",
       " \"'d\",\n",
       " 'be',\n",
       " 'surprised',\n",
       " '!']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/nlp/lib/python3.11/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "I."
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP environment (nlp)",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
